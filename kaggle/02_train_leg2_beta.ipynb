{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "48e4f37c",
            "metadata": {},
            "source": [
                "# Leg 2: News Model Training\n",
                "\n",
                "This notebook is an optimized version of `02_train_leg2_news.ipynb`\n",
                "\n",
                "\n",
                "**Optimizations**:\n",
                "1. **Device**: Uses `mps` (Metal Performance Shaders) for GPU acceleration\n",
                "2. **Batch Size**: Increased to 128\n",
                "3. **Epochs**: Increased to 10\n",
                "4. **Data Loading**: Increased `num_workers` to 8 and enabled `pin_memory`\n",
                "\n",
                "**Workflow**:\n",
                "1. Load preprocessed news data\n",
                "2. Wrap the PyTorch model (`Leg2HANWrapper`) with MPS support\n",
                "3. Run Walk-Forward Cross-Validation\n",
                "4. Save raw OOF predictions (`/kaggle/working/leg2_oof_preds.pkl`)\n",
                "5. Evaluate raw performance\n",
                "6. Upload results to Kaggle Hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18509070",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "import ast\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from scipy.stats import pearsonr, spearmanr\n",
                "\n",
                "# Added for Kaggle Upload\n",
                "import kagglehub\n",
                "from datetime import datetime\n",
                "\n",
                "REPO_URL = \"https://github.com/brianrp09232000/multimodal-eq-sizing.git\"\n",
                "REPO_DIR = \"/kaggle/working/multimodal-eq-sizing\"\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    print(f\"Cloning {REPO_URL} into {REPO_DIR}...\")\n",
                "    !git clone {REPO_URL} {REPO_DIR}\n",
                "else:\n",
                "    print(\"Repository already exists. Pulling latest changes...\")\n",
                "    !cd {REPO_DIR} && git pull\n",
                "\n",
                "print(\"Fixing protobuf version...\")\n",
                "!pip install \"protobuf==3.20.3\" \n",
                "\n",
                "print(\"Installing requirements...\")\n",
                "!pip install -r {REPO_DIR}/requirements.txt\n",
                "\n",
                "src_path = os.path.join(REPO_DIR, \"src\")\n",
                "if os.path.exists(src_path):\n",
                "    print(f\"src folder found at: {src_path}\")\n",
                "    if src_path not in sys.path:\n",
                "        sys.path.append(src_path)\n",
                "else:\n",
                "    print(\"folder MISSING. Check git clone output.\")\n",
                "\n",
                "# Add source to path for imports\n",
                "current_dir = Path(os.getcwd())\n",
                "# Try standard Kaggle repo path first, then local fallback\n",
                "kaggle_repo_path = Path(\"/kaggle/working/multimodal-eq-sizing/src\")\n",
                "local_repo_path = current_dir.parent / 'src'\n",
                "\n",
                "if kaggle_repo_path.exists():\n",
                "    sys.path.append(str(kaggle_repo_path.parent))\n",
                "    sys.path.append(str(kaggle_repo_path))\n",
                "elif local_repo_path.exists() and str(local_repo_path) not in sys.path:\n",
                "    sys.path.append(str(local_repo_path))\n",
                "\n",
                "# Project Imports\n",
                "try:\n",
                "    from src.models.HAN_l2 import FinbertHAN\n",
                "    from src.data.loaders import NewsDataset, han_collate_fn\n",
                "    from src.utils.cv import generate_yearly_oof\n",
                "except ImportError:\n",
                "    # Fallback if src is directly in path\n",
                "    from models.HAN_l2 import FinbertHAN\n",
                "    from data.loaders import NewsDataset, han_collate_fn\n",
                "    from utils.cv import generate_yearly_oof\n",
                "\n",
                "if torch.backends.mps.is_available():\n",
                "    DEVICE = torch.device(\"mps\")\n",
                "    print(\"Using Apple MPS acceleration\")\n",
                "elif torch.cuda.is_available():\n",
                "    DEVICE = torch.device(\"cuda\")\n",
                "    print(\"Using CUDA acceleration\")\n",
                "else:\n",
                "    DEVICE = torch.device(\"cpu\")\n",
                "    print(\"Using CPU\")\n",
                "\n",
                "SEED = 42\n",
                "BATCH_SIZE = 128 \n",
                "EPOCHS = 10 \n",
                "LR = 1e-4\n",
                "MAX_GRAD_NORM = 1.0\n",
                "NUM_WORKERS = 8\n",
                "PIN_MEMORY = True\n",
                "# Set seeds\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "\n",
                "print(f\"Device: {DEVICE}\")\n",
                "print(f\"Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"Epochs: {EPOCHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2a958f3",
            "metadata": {},
            "source": [
                "## 1. Define Model Wrapper\n",
                "\n",
                "We update the wrapper to use `pin_memory` and `NUM_WORKERS` in the DataLoader."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b063599f",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Leg2HANWrapper:\n",
                "    \"\"\"\n",
                "    Scikit-learn compatible wrapper for the FinbertHAN PyTorch model\n",
                "    Produces raw regression outputs\n",
                "    Optimized for Apple Silicon via DataLoader params\n",
                "    \"\"\"\n",
                "    def __init__(self, \n",
                "                 batch_size=BATCH_SIZE, \n",
                "                 epochs=EPOCHS, \n",
                "                 lr=LR, \n",
                "                 device=DEVICE):\n",
                "        self.batch_size = batch_size\n",
                "        self.epochs = epochs\n",
                "        self.lr = lr\n",
                "        self.device = device\n",
                "        self.model = None\n",
                "        self.train_loss_history = []\n",
                "\n",
                "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
                "        # Prepare data\n",
                "        train_df = X.copy()\n",
                "        if y is not None:\n",
                "            train_df['target'] = y.values\n",
                "        \n",
                "        dataset = NewsDataset(train_df)\n",
                "        loader = DataLoader(\n",
                "            dataset, \n",
                "            batch_size=self.batch_size, \n",
                "            shuffle=True, \n",
                "            collate_fn=han_collate_fn,\n",
                "            num_workers=NUM_WORKERS,\n",
                "            pin_memory=PIN_MEMORY\n",
                "        )\n",
                "\n",
                "        self.model = FinbertHAN(aux_dim=4).to(self.device)\n",
                "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n",
                "        criterion = nn.MSELoss()\n",
                "\n",
                "        self.model.train()\n",
                "        self.train_loss_history = []\n",
                "\n",
                "        for epoch in range(self.epochs):\n",
                "            epoch_loss = 0.0\n",
                "            count = 0\n",
                "            for batch in loader:\n",
                "                input_ids = batch['input_ids'].to(self.device)\n",
                "                att_mask = batch['attention_mask'].to(self.device)\n",
                "                doc_lens = batch['doc_lengths']\n",
                "                time_gaps = batch['time_gaps'].to(self.device)\n",
                "                aux_feats = batch['aux_features'].to(self.device)\n",
                "                news_mask = batch['news_mask'].to(self.device)\n",
                "                targets = batch['targets'].to(self.device)\n",
                "\n",
                "                optimizer.zero_grad()\n",
                "                preds, _, _ = self.model(input_ids, att_mask, doc_lens, time_gaps, aux_feats, news_mask)\n",
                "                loss = criterion(preds, targets)\n",
                "                loss.backward()\n",
                "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), MAX_GRAD_NORM)\n",
                "                optimizer.step()\n",
                "\n",
                "                epoch_loss += loss.item() * targets.size(0)\n",
                "                count += targets.size(0)\n",
                "            \n",
                "            avg_loss = epoch_loss / max(1, count)\n",
                "            self.train_loss_history.append(avg_loss)\n",
                "            print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.6f}\")\n",
                "        return self\n",
                "\n",
                "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
                "        self.model.eval()\n",
                "        test_df = X.copy()\n",
                "        if 'target' not in test_df.columns:\n",
                "            test_df['target'] = 0.0 \n",
                "            \n",
                "        dataset = NewsDataset(test_df)\n",
                "        loader = DataLoader(\n",
                "            dataset, \n",
                "            batch_size=self.batch_size, \n",
                "            shuffle=False, \n",
                "            collate_fn=han_collate_fn,\n",
                "            num_workers=NUM_WORKERS,\n",
                "            pin_memory=PIN_MEMORY\n",
                "        )\n",
                "\n",
                "        all_preds = []\n",
                "        with torch.no_grad():\n",
                "            for batch in loader:\n",
                "                input_ids = batch['input_ids'].to(self.device)\n",
                "                att_mask = batch['attention_mask'].to(self.device)\n",
                "                doc_lens = batch['doc_lengths']\n",
                "                time_gaps = batch['time_gaps'].to(self.device)\n",
                "                aux_feats = batch['aux_features'].to(self.device)\n",
                "                news_mask = batch['news_mask'].to(self.device)\n",
                "                preds, _, _ = self.model(input_ids, att_mask, doc_lens, time_gaps, aux_feats, news_mask)\n",
                "                all_preds.append(preds.cpu().numpy())\n",
                "        \n",
                "        return np.concatenate(all_preds).flatten()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9781b5b0",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "\n",
                "Same loading logic as the standard notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de4d319e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define search directories for Kaggle\n",
                "SEARCH_DIRS = [\n",
                "    \"/kaggle/input\", \n",
                "    \"/kaggle/working\", \n",
                "    \"../src/data/datasets\" # Fallback for local\n",
                "]\n",
                "\n",
                "def find_file(filename, search_dirs):\n",
                "    print(f\"Searching for {filename}...\")\n",
                "    for directory in search_dirs:\n",
                "        # Check direct path\n",
                "        path = Path(directory) / filename\n",
                "        if path.exists():\n",
                "            return path\n",
                "        # Check recursive\n",
                "        if Path(directory).exists():\n",
                "            try:\n",
                "                matches = list(Path(directory).rglob(filename))\n",
                "                if matches:\n",
                "                    return matches[0]\n",
                "            except Exception as e:\n",
                "                print(f\"Access error in {directory}: {e}\")\n",
                "    return None\n",
                "\n",
                "NEWS_FILENAME = \"filtered_news_dataset.csv\"\n",
                "PRICES_FILENAME = \"prices_dataset.csv\"\n",
                "\n",
                "NEWS_PATH = find_file(NEWS_FILENAME, SEARCH_DIRS)\n",
                "PRICES_PATH = find_file(PRICES_FILENAME, SEARCH_DIRS)\n",
                "\n",
                "def robust_literal_eval(val):\n",
                "    if pd.isna(val) or val == \"\" or str(val).lower() == 'nan':\n",
                "        return []\n",
                "    try:\n",
                "        return ast.literal_eval(val)\n",
                "    except (ValueError, SyntaxError):\n",
                "        return [str(val)]\n",
                "\n",
                "if NEWS_PATH:\n",
                "    print(f\"Loading News from {NEWS_PATH}...\")\n",
                "    news_converters = {\n",
                "        'sentences': robust_literal_eval,\n",
                "        'Article_title': robust_literal_eval,\n",
                "        'entities': robust_literal_eval,\n",
                "        'entities_today': robust_literal_eval\n",
                "    }\n",
                "    df_news = pd.read_csv(NEWS_PATH, converters=news_converters)\n",
                "    \n",
                "    if 'sentences' in df_news.columns and 'Article_title' not in df_news.columns:\n",
                "        df_news = df_news.rename(columns={'sentences': 'Article_title'})\n",
                "    if 'Stock_symbol' in df_news.columns and 'ticker' not in df_news.columns:\n",
                "        df_news = df_news.rename(columns={'Stock_symbol': 'ticker'})\n",
                "    df_news['Date'] = pd.to_datetime(df_news['Date'], utc=True)\n",
                "    print(f\"News Loaded. Shape: {df_news.shape}.\")\n",
                "else:\n",
                "    raise FileNotFoundError(f\"News file not found in {SEARCH_DIRS}\")\n",
                "\n",
                "\n",
                "if PRICES_PATH:\n",
                "    print(f\"Loading Prices from {PRICES_PATH}...\")\n",
                "    df_prices = pd.read_csv(PRICES_PATH)\n",
                "    df_prices['Date'] = pd.to_datetime(df_prices['Date'], utc=True)\n",
                "    print(f\"Prices Loaded. Shape: {df_prices.shape}\")\n",
                "else:\n",
                "    raise FileNotFoundError(f\"Prices file not found in {SEARCH_DIRS}\")\n",
                "\n",
                "df_full = pd.merge(df_prices, df_news, on=['Date', 'ticker'], how='left')\n",
                "df_full = df_full.sort_values('Date').reset_index(drop=True)\n",
                "\n",
                "target_col = 'excess_return'\n",
                "\n",
                "if target_col in df_full.columns:\n",
                "    df_full = df_full.dropna(subset=[target_col])\n",
                "    X = df_full.drop(columns=[target_col])\n",
                "    y = df_full[target_col]\n",
                "    dates = df_full['Date']\n",
                "    print(f\"Data Ready. Final Shape: {df_full.shape}\")\n",
                "else:\n",
                "    raise KeyError(f\"Target column '{target_col}' not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "96eb85f7",
            "metadata": {},
            "source": [
                "## 3. Walk-Forward Cross Validation (OOF Generation)\n",
                "\n",
                "Uses `generate_yearly_oof` with the optimized factory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59520544",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the optimized model factory\n",
                "def model_factory():\n",
                "    return Leg2HANWrapper(batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, device=DEVICE)\n",
                "\n",
                "# Run OOF Generation\n",
                "# We rely on the speedup inside the model training for performance gains\n",
                "if 'X' in locals():\n",
                "    oof_preds_raw, oof_targets, fold_stats = generate_yearly_oof(\n",
                "        model_factory=model_factory,\n",
                "        X=X,\n",
                "        y=y,\n",
                "        dates=dates,\n",
                "        min_train_years=2,\n",
                "        n_jobs=1 \n",
                "    )\n",
                "    \n",
                "    print(f\"OOF Prediction Complete. Generated {len(oof_preds_raw)} predictions.\")\n",
                "    print(pd.DataFrame(fold_stats))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0a61d1e2",
            "metadata": {},
            "source": [
                "## 4. Save Raw OOF Predictions\n",
                "\n",
                "Saves to `/kaggle/working/leg2_oof_preds.pkl` - this will overwrite any existing file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76c5eb64",
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'oof_preds_raw' in locals() and len(oof_preds_raw) > 0:\n",
                "    # 1. Reconstruct Metadata (Date, ticker)\n",
                "    unique_years = sorted(dates.dt.year.unique())\n",
                "    min_train_years = 2\n",
                "    val_years = unique_years[min_train_years:]\n",
                "    \n",
                "    oof_dfs = []\n",
                "    \n",
                "    for val_year in val_years:\n",
                "        val_mask = dates.dt.year == val_year\n",
                "        subset = df_full.loc[val_mask, ['Date', 'ticker', target_col]].copy()\n",
                "        oof_dfs.append(subset)\n",
                "        \n",
                "    oof_df = pd.concat(oof_dfs)\n",
                "    oof_df['prediction'] = oof_preds_raw\n",
                "    \n",
                "    print(f\"OOF Metadata Shape: {oof_df.shape}\")\n",
                "    print(f\"OOF Preds Length: {len(oof_preds_raw)}\")\n",
                "    assert len(oof_df) == len(oof_preds_raw), \"Mismatch between OOF metadata and predictions length\"\n",
                "    \n",
                "    # 3. Save to Kaggle Working Directory\n",
                "    SAVE_PATH = \"/kaggle/working/leg2_oof_preds.pkl\"\n",
                "    if not os.access(os.path.dirname(SAVE_PATH), os.W_OK):\n",
                "        # Fallback to local\n",
                "        SAVE_PATH = \"../data/leg2_oof_preds.pkl\"\n",
                "    \n",
                "    os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
                "    \n",
                "    oof_df.to_pickle(SAVE_PATH)\n",
                "    print(f\"Saved OOF predictions to: {SAVE_PATH}\")\n",
                "    print(oof_df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "75cb90e9",
            "metadata": {},
            "source": [
                "## 5. Raw Performance Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d92cde88",
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'oof_preds_raw' in locals() and len(oof_preds_raw) > 0:\n",
                "    mse_raw = mean_squared_error(oof_targets, oof_preds_raw)\n",
                "    ic_raw, _ = pearsonr(oof_preds_raw, oof_targets)\n",
                "    \n",
                "    print(f\"Leg 2 Results\")\n",
                "    print(f\"MSE Raw: {mse_raw:.6f}\")\n",
                "    print(f\"IC Raw: {ic_raw:.4f}\")\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    indices = np.random.choice(len(oof_targets), min(5000, len(oof_targets)), replace=False)\n",
                "    \n",
                "    sns.regplot(x=oof_preds_raw[indices], y=oof_targets[indices], \n",
                "                scatter_kws={'alpha':0.3, 's': 10}, line_kws={'color':'red'})\n",
                "    plt.title(f\"Raw Scores vs Excess Return (IC={ic_raw:.3f})\")\n",
                "    plt.xlabel(\"Raw NN Output\")\n",
                "    plt.ylabel(\"True Excess Return\")\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5721342",
            "metadata": {},
            "outputs": [],
            "source": [
                "handle = \"iinarixf0x/leg2-news-model\"\n",
                "local_dataset_dir = f\"/kaggle/working/\"\n",
                "current_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
                "\n",
                "try:\n",
                "    kagglehub.dataset_upload(handle, local_dataset_dir, version_notes=f\"Dataset {current_date}\")\n",
                "    print(\"Upload successful\")\n",
                "except Exception as e:\n",
                "    print(f\"Upload failed: {e}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
