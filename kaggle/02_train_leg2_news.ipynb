{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Leg 2: News Model Training\n",
                "\n",
                "This notebook trains the **Leg 2 News** component of the multimodal pipeline.\n",
                "\n",
                "**Architecture**:\n",
                "1. **FinbertHAN**: Hierarchical Attention Network using frozen DistilRoBERTa embeddings -> Bi-GRU -> Time-Aware Attention\n",
                "2. **Inputs**: Tokenized headlines, time gaps, and auxiliary features (Velocity, Novelty, Event Flags)\n",
                "3. **Target**: Next-day Open-to-Close Excess Return\n",
                "4. **Calibration**: Isotonic Regression on out of fold OOF predictions\n",
                "\n",
                "**Process**:\n",
                "1. Load preprocessed news data which contains sentences, gaps\n",
                "2. Wrap the PyTorch model in a scikit-learn compatible class (`Leg2HANWrapper`)\n",
                "3. Run walk-forward cross-validation aka the expanding window using `src.utils.cv`\n",
                "4. Calibrate raw scores\n",
                "5. Evaluate performance (MSE, IC) and visualize results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/nchavez/Projects/school/Classes/COMP647DeepLearning/multimodal-eq-sizing/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on cpu\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from scipy.stats import pearsonr, spearmanr\n",
                "\n",
                "current_dir = Path(os.getcwd())\n",
                "src_path = current_dir.parent / 'src'\n",
                "if str(src_path) not in sys.path:\n",
                "    sys.path.append(str(src_path))\n",
                "\n",
                "from models.HAN_l2 import FinbertHAN\n",
                "from data.loaders import NewsDataset, han_collate_fn\n",
                "from utils.cv import generate_yearly_oof\n",
                "from models.calibrators import IsotonicCalibrator\n",
                "\n",
                "# Config\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "SEED = 42\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 5  # Keeping low for testing, for better results increase it, no more than 512 though\n",
                "LR = 1e-4\n",
                "MAX_GRAD_NORM = 1.0\n",
                "\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "\n",
                "print(f\"Running on {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Define Model Wrapper\n",
                "\n",
                "To utilize the `generate_yearly_oof` function from `src.utils.cv` (which expects a standard `.fit()` / `.predict()` interface), we wrap the PyTorch training logic into a class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Leg2HANWrapper:\n",
                "    \"\"\"\n",
                "    Scikit-learn compatible wrapper for the FinbertHAN PyTorch model\n",
                "    Handles DataLoaders, Training Loops, and GPU movement internally\n",
                "    \"\"\"\n",
                "    def __init__(self, \n",
                "                 batch_size=BATCH_SIZE, \n",
                "                 epochs=EPOCHS, \n",
                "                 lr=LR, \n",
                "                 device=DEVICE):\n",
                "        self.batch_size = batch_size\n",
                "        self.epochs = epochs\n",
                "        self.lr = lr\n",
                "        self.device = device\n",
                "        self.model = None\n",
                "        self.train_loss_history = []\n",
                "\n",
                "    def fit(self, X: pd.DataFrame, y: pd.Series = None):\n",
                "        \"\"\"\n",
                "        Trains the FinbertHAN model\n",
                "        X: DataFrame containing 'sentences', 'gaps', 'novelty'\n",
                "        y: Target series ie the excess return If None, checks if 'target' is in X\n",
                "        \"\"\"\n",
                "        # Prepare data\n",
                "        train_df = X.copy()\n",
                "        if y is not None:\n",
                "            train_df['target'] = y.values\n",
                "        \n",
                "        # Initialize dataset & loader\n",
                "        dataset = NewsDataset(train_df)\n",
                "        loader = DataLoader(\n",
                "            dataset, \n",
                "            batch_size=self.batch_size, \n",
                "            shuffle=True, \n",
                "            collate_fn=han_collate_fn,\n",
                "            num_workers=0 # 0 is default for Windows, if on Linux/Apple silicon set >0 for speed\n",
                "        )\n",
                "\n",
                "        # Initialize Model\n",
                "        self.model = FinbertHAN(aux_dim=4).to(self.device)\n",
                "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.lr)\n",
                "        criterion = nn.MSELoss()\n",
                "\n",
                "        self.model.train()\n",
                "        self.train_loss_history = []\n",
                "\n",
                "        # Training loop\n",
                "        for epoch in range(self.epochs):\n",
                "            epoch_loss = 0.0\n",
                "            count = 0\n",
                "            for batch in loader:\n",
                "                # Move batch to device\n",
                "                input_ids = batch['input_ids'].to(self.device)\n",
                "                att_mask = batch['attention_mask'].to(self.device)\n",
                "                doc_lens = batch['doc_lengths']\n",
                "                time_gaps = batch['time_gaps'].to(self.device)\n",
                "                aux_feats = batch['aux_features'].to(self.device)\n",
                "                news_mask = batch['news_mask'].to(self.device)\n",
                "                targets = batch['targets'].to(self.device)\n",
                "\n",
                "                optimizer.zero_grad()\n",
                "\n",
                "                # Forward pass\n",
                "                preds, _, _ = self.model(\n",
                "                    input_ids, att_mask, doc_lens, time_gaps, aux_feats, news_mask\n",
                "                )\n",
                "\n",
                "                loss = criterion(preds, targets)\n",
                "                loss.backward()\n",
                "                \n",
                "                # Gradient Clipping for stability\n",
                "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), MAX_GRAD_NORM)\n",
                "                \n",
                "                optimizer.step()\n",
                "\n",
                "                epoch_loss += loss.item() * targets.size(0)\n",
                "                count += targets.size(0)\n",
                "            \n",
                "            avg_loss = epoch_loss / max(1, count)\n",
                "            self.train_loss_history.append(avg_loss)\n",
                "            # Print progress\n",
                "            print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {avg_loss:.6f}\")\n",
                "        \n",
                "        return self\n",
                "\n",
                "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        Generates predictions for X\n",
                "        \"\"\"\n",
                "        self.model.eval()\n",
                "        \n",
                "        # Prepare Data \n",
                "        test_df = X.copy()\n",
                "        if 'target' not in test_df.columns:\n",
                "            test_df['target'] = 0.0 # Dummy target, so this needs to be changed when running\n",
                "            \n",
                "        dataset = NewsDataset(test_df)\n",
                "        loader = DataLoader(\n",
                "            dataset, \n",
                "            batch_size=self.batch_size, \n",
                "            shuffle=False, \n",
                "            collate_fn=han_collate_fn,\n",
                "            num_workers=0\n",
                "        )\n",
                "\n",
                "        all_preds = []\n",
                "        with torch.no_grad():\n",
                "            for batch in loader:\n",
                "                input_ids = batch['input_ids'].to(self.device)\n",
                "                att_mask = batch['attention_mask'].to(self.device)\n",
                "                doc_lens = batch['doc_lengths']\n",
                "                time_gaps = batch['time_gaps'].to(self.device)\n",
                "                aux_feats = batch['aux_features'].to(self.device)\n",
                "                news_mask = batch['news_mask'].to(self.device)\n",
                "\n",
                "                preds, _, _ = self.model(\n",
                "                    input_ids, att_mask, doc_lens, time_gaps, aux_feats, news_mask\n",
                "                )\n",
                "                all_preds.append(preds.cpu().numpy())\n",
                "        \n",
                "        # Flatten results\n",
                "        return np.concatenate(all_preds).flatten()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f5f8ba4e",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "\n",
                "We load the dataset processed news data. We check multiple potential locations as this might run in a Kaggle kernel or local environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "861430e2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ERROR: Data file not found...Checked paths: ['../data/processed/news_ready.pkl', '/kaggle/input/multimodal-leg2-data/news_ready.pkl', '/kaggle/working/news_ready.pkl', 'news_ready.pkl']\n"
                    ]
                },
                {
                    "ename": "FileNotFoundError",
                    "evalue": "news_ready.pkl not found in ['../data/processed/news_ready.pkl', '/kaggle/input/multimodal-leg2-data/news_ready.pkl', '/kaggle/working/news_ready.pkl', 'news_ready.pkl']",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: Data file not found...Checked paths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEARCH_PATHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnews_ready.pkl not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSEARCH_PATHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: news_ready.pkl not found in ['../data/processed/news_ready.pkl', '/kaggle/input/multimodal-leg2-data/news_ready.pkl', '/kaggle/working/news_ready.pkl', 'news_ready.pkl']"
                    ]
                }
            ],
            "source": [
                "# Checking multiple locations\n",
                "# Change this accoring to however you have your local machine setup because I have been changing\n",
                "# it a lot for the past few hours\n",
                "SEARCH_PATHS = [\n",
                "    \"../data/processed/news_ready.pkl\",\n",
                "    \"/kaggle/input/multimodal-leg2-data/news_ready.pkl\",\n",
                "    \"/kaggle/working/news_ready.pkl\",\n",
                "    \"news_ready.pkl\"\n",
                "]\n",
                "\n",
                "data_found = False\n",
                "for path in SEARCH_PATHS:\n",
                "    if os.path.exists(path):\n",
                "        DATA_PATH = path\n",
                "        data_found = True\n",
                "        break\n",
                "        \n",
                "if data_found:\n",
                "    print(f\"Loading data from {DATA_PATH}\")\n",
                "    df_full = pd.read_pickle(DATA_PATH)\n",
                "    \n",
                "    # Date format\n",
                "    df_full['Date'] = pd.to_datetime(df_full['Date'], utc=True)\n",
                "    df_full = df_full.sort_values('Date').reset_index(drop=True)\n",
                "    \n",
                "    # Define Features and Target\n",
                "    # 'sentences' and 'gaps' are lists, passed as objects\n",
                "    # We pass the whole dataframe to the wrapper, but split X and y for the CV utility\n",
                "    target_col = 'excess_return'\n",
                "    \n",
                "    # Filter out rows with NaN targets if any\n",
                "    df_full = df_full.dropna(subset=[target_col])\n",
                "    \n",
                "    X = df_full.drop(columns=[target_col])\n",
                "    y = df_full[target_col]\n",
                "    dates = df_full['Date']\n",
                "    \n",
                "    print(f\"Data Loaded. Shape: {df_full.shape}\")\n",
                "    print(f\"Date Range: {dates.min()} to {dates.max()}\")\n",
                "else:\n",
                "    print(f\"ERROR: Data file not found...Checked paths: {SEARCH_PATHS}\")\n",
                "    raise FileNotFoundError(f\"news_ready.pkl not found in {SEARCH_PATHS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Walk-Forward Cross Validation (OOF Generation)\n",
                "\n",
                "We uses `generate_yearly_oof` to simulate a realistic production environment.\n",
                "- **Expanding Window**: Train on years `[Start, Y-1]`, predict on `Y`\n",
                "- **Min Train Years**: 2 Ensures we have enough history for the first fold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the model factory returns a fresh instance\n",
                "def model_factory():\n",
                "    return Leg2HANWrapper(batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR, device=DEVICE)\n",
                "\n",
                "# Run OOF Generation\n",
                "# This will take a while\n",
                "if 'X' in locals():\n",
                "    oof_preds_raw, oof_targets, fold_stats = generate_yearly_oof(\n",
                "        model_factory=model_factory,\n",
                "        X=X,\n",
                "        y=y,\n",
                "        dates=dates,\n",
                "        min_train_years=2,\n",
                "        n_jobs=1  # 1 for neural nets\n",
                "    )\n",
                "    \n",
                "    print(f\"OOF Prediction Complete. Generated {len(oof_preds_raw)} predictions.\")\n",
                "    \n",
                "    # Display fold stats\n",
                "    print(pd.DataFrame(fold_stats))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Calibration\n",
                "\n",
                "The raw output of the Neural Network (MSE regression) might not align perfectly with probabilities or expected returns in a linear fashion. We use Isotonic Regression to map the raw scores to calibrated Expected Excess Returns. We are still using the IsotonicCalibrator class from the previous notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'oof_preds_raw' in locals() and len(oof_preds_raw) > 0:\n",
                "    # Initialize and fit calibrator\n",
                "    # Training Isotonic requires strictly monotonic relationship if possible\n",
                "    calibrator = IsotonicCalibrator(out_of_bounds='clip')\n",
                "    calibrator.fit(oof_preds_raw, oof_targets)\n",
                "    \n",
                "    # Generate calibrated scores\n",
                "    oof_preds_calib = calibrator.predict(oof_preds_raw)\n",
                "    \n",
                "    # Save calibrator for inference\n",
                "    save_path = Path('../models/leg2_calibrator.pkl')\n",
                "    calibrator.save(save_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Metrics & Visualization\n",
                "\n",
                "We evaluate the model using:\n",
                "1. **MSE**: Mean Squared Error (Lower is better)\n",
                "2. **IC (Information Coefficient)**: Pearson correlation between prediction and target (Higher is better)\n",
                "3. **Visuals**: Scatter plot of Raw vs Target and Calibrated vs Target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'oof_preds_raw' in locals() and len(oof_preds_raw) > 0:\n",
                "    # 1. Metrics\n",
                "    mse_raw = mean_squared_error(oof_targets, oof_preds_raw)\n",
                "    mse_calib = mean_squared_error(oof_targets, oof_preds_calib)\n",
                "    \n",
                "    ic_raw, _ = pearsonr(oof_preds_raw, oof_targets)\n",
                "    ic_calib, _ = pearsonr(oof_preds_calib, oof_targets)\n",
                "    \n",
                "    print(f\"Leg 2 Results\")\n",
                "    print(f\"MSE (Raw): {mse_raw:.6f}\")\n",
                "    print(f\"MSE (Calib): {mse_calib:.6f}\")\n",
                "    print(f\"IC (Raw): {ic_raw:.4f}\")\n",
                "    print(f\"IC (Calib): {ic_calib:.4f}\")\n",
                "    \n",
                "    # 2. Visualization\n",
                "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
                "    \n",
                "    # Plot 1: Raw Predictions vs Target chose binning for clarity\n",
                "    # Using sample if data is huge for speed\n",
                "    indices = np.random.choice(len(oof_targets), min(5000, len(oof_targets)), replace=False)\n",
                "    \n",
                "    sns.regplot(x=oof_preds_raw[indices], y=oof_targets[indices], ax=ax[0], \n",
                "                scatter_kws={'alpha':0.3, 's': 10}, line_kws={'color':'red'})\n",
                "    ax[0].set_title(f\"Raw Scores vs Excess Return (IC={ic_raw:.3f})\")\n",
                "    ax[0].set_xlabel(\"Raw NN Output\")\n",
                "    ax[0].set_ylabel(\"True Excess Return\")\n",
                "    \n",
                "    # Plot 2: Calibrated vs Target\n",
                "    sns.regplot(x=oof_preds_calib[indices], y=oof_targets[indices], ax=ax[1], \n",
                "                scatter_kws={'alpha':0.3, 's': 10}, line_kws={'color':'green'})\n",
                "    ax[1].set_title(f\"Calibrated Scores vs Excess Return (IC={ic_calib:.3f})\")\n",
                "    ax[1].set_xlabel(\"Calibrated Probability/Return\")\n",
                "    ax[1].set_ylabel(\"True Excess Return\")\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
