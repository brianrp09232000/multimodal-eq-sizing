{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5549253,"sourceType":"datasetVersion","datasetId":2976522},{"sourceId":13920741,"sourceType":"datasetVersion","datasetId":8870532},{"sourceId":13965676,"sourceType":"datasetVersion","datasetId":8902694}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clone GitHub Repo and Install Dependencies ","metadata":{}},{"cell_type":"code","source":"import os\n\n!rm -rf multimodal-eq-sizing\n!git clone https://github.com/brianrp09232000/multimodal-eq-sizing.git\n!pip install -r multimodal-eq-sizing/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:07:50.594203Z","iopub.execute_input":"2025-12-07T18:07:50.595015Z","iopub.status.idle":"2025-12-07T18:07:54.988662Z","shell.execute_reply.started":"2025-12-07T18:07:50.594948Z","shell.execute_reply":"2025-12-07T18:07:54.987821Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Cloning into 'multimodal-eq-sizing'...\nremote: Enumerating objects: 843, done.\u001b[K\nremote: Counting objects: 100% (162/162), done.\u001b[K\nremote: Compressing objects: 100% (69/69), done.\u001b[K\nremote: Total 843 (delta 123), reused 93 (delta 93), pack-reused 681 (from 3)\u001b[K\nReceiving objects: 100% (843/843), 886.58 KiB | 14.78 MiB/s, done.\nResolving deltas: 100% (518/518), done.\nRequirement already satisfied: yfinance==0.2.66 in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 1)) (0.2.66)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 2)) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 3)) (1.26.4)\nRequirement already satisfied: datetime in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 4)) (6.0)\nRequirement already satisfied: dataclasses in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 5)) (0.6)\nRequirement already satisfied: typing in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 6)) (3.7.4.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 7)) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 8)) (4.53.3)\nRequirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.32.5)\nRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (0.0.12)\nRequirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (4.5.0)\nRequirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.4.6)\nRequirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (3.18.2)\nRequirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (4.13.4)\nRequirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (0.12.0)\nRequirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (6.33.0)\nRequirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (15.0.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r multimodal-eq-sizing/requirements.txt (line 2)) (2.9.0.post0)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r multimodal-eq-sizing/requirements.txt (line 2)) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2.4.1)\nRequirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from datetime->-r multimodal-eq-sizing/requirements.txt (line 4)) (8.1.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (0.36.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.7)\nRequirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2025.10.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r multimodal-eq-sizing/requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.23)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import sys\nimport pathlib\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T20:40:30.329573Z","iopub.execute_input":"2025-12-07T20:40:30.329882Z","iopub.status.idle":"2025-12-07T20:40:31.098553Z","shell.execute_reply.started":"2025-12-07T20:40:30.329855Z","shell.execute_reply":"2025-12-07T20:40:31.097906Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"np.seterr(invalid=\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T20:40:37.702477Z","iopub.execute_input":"2025-12-07T20:40:37.703241Z","iopub.status.idle":"2025-12-07T20:40:37.708749Z","shell.execute_reply.started":"2025-12-07T20:40:37.703217Z","shell.execute_reply":"2025-12-07T20:40:37.708150Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Uses the current directory where the notebook is running\nrepo_root = pathlib.Path(\"multimodal-eq-sizing\")\nsys.path.append(str(repo_root.resolve())) # .resolve() gets the full absolute path locally","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T20:40:42.426225Z","iopub.execute_input":"2025-12-07T20:40:42.426510Z","iopub.status.idle":"2025-12-07T20:40:42.431210Z","shell.execute_reply.started":"2025-12-07T20:40:42.426488Z","shell.execute_reply":"2025-12-07T20:40:42.430267Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from src.data.loaders import (\n    get_tickers_history,\n    get_return_data,\n    get_excess_return,\n    get_vix_data,\n    get_spread_z,\n    get_sector_map,\n    get_adv_dollar\n)\n\nfrom src.data.features.price_features import calculate_leg_one_features\nfrom src.data.features.news_features import built_news_features\nfrom src.data.universe import tickers_with_most_headlines\nfrom src.data.universe import tickers_with_most_headlines","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T20:41:18.339082Z","iopub.execute_input":"2025-12-07T20:41:18.340157Z","iopub.status.idle":"2025-12-07T20:41:18.344560Z","shell.execute_reply.started":"2025-12-07T20:41:18.340123Z","shell.execute_reply":"2025-12-07T20:41:18.343795Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_colwidth', None)\n# pd.set_option('display.max_rows', 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions Definition","metadata":{}},{"cell_type":"code","source":"def count_headlines_per_ticker(news_df, start=None, end=None):\n    \"\"\"Counts the number of headlines for each ticker symbol \n    Input: news_df pandas dataframe with ticker column for ticker symbols\n    Output: pandas dataframe containing two columns: ticker names and the \n                number of headlines for the ticker\"\"\"\n    \n    #check columns in dataframe\n    columns = list(news_df.columns)\n    if (('date' not in columns) and ('Date' not in columns)) or (('ticker' not in columns) and ('Stock_symbol' not in columns)):\n        print('input dataframe does not have both ticker and date columns')\n        return pd.DataFrame()\n    \n    #find column names\n    date_col = 'date' if 'date' in columns else 'Date'\n    ticker_col = 'ticker' if 'ticker' in columns else 'Stock_symbol'\n\n    #filter dates\n    if start is not None: \n        start_filter = news_df[date_col] >= str(start)\n        news_df = news_df[start_filter]\n    if end is not None: \n        end_filter = news_df[date_col] <= str(end)\n        news_df = news_df[end_filter]\n    \n    # Count occurrences in a specific column\n    headline_counts = news_df[ticker_col].value_counts()\n    df = headline_counts.to_frame(name='count')\n    df['ticker'] = list(df.index)\n    df = df.reset_index(drop=True)\n    \n    return df[['ticker','count']]\n\n\n\ndef tickers_with_most_headlines(news_df, start=None, end=None, n=200):\n    \"\"\"Finds the tickers with the most headlines \n    Input: news_df pandas dataframe with ticker column for ticker symbols\n            optional: n interger, number of top tickers to return\n    Output: list containing the number of headlines per ticker\n                for the tickers with the most headlines\"\"\"\n    \n    #count headlines for each ticker\n    df = count_headlines_per_ticker(news_df, start, end)\n\n    #limit dataframe to n tickers\n    df = df.sort_values(['count'], ascending=False)\n    df = df[:n]\n    df.reset_index(drop=True, inplace=True)\n    \n    return df\n\n\ndef get_date_range(df: pd.DataFrame) -> tuple:\n    grouped_by_date = df.groupby([\"ticker\"]).agg(['min', 'max', 'count'])[\"Date\"]\n    start = grouped_by_date[\"min\"].min()\n    end = grouped_by_date[\"max\"].max()\n    return start, end\n\n\ndef add_sector(df):\n    tickers = df[\"ticker\"].unique()\n    sector_map = get_sector_map(tickers)\n    df = df.join(sector_map, on=\"ticker\")\n    \n    # remove None values in 'sector', these are ETFs not stocks\n    df = df.dropna(subset=['sector'])\n    return df\n\n\ndef add_excess_return(df, start, end):\n    excess_return_df = get_excess_return(df, start, end)\n    df = df.merge(excess_return_df, on=[\"ticker\", \"Date\"], how=\"left\")\n    return df\n\n\ndef add_vix_z(df, start, end):\n    vix_z_df = get_vix_data(start, end)\n    format_str = \"%Y-%m-%d\"\n    vix_z_df[\"Date\"] = vix_z_df[\"Date\"].dt.strftime(format_str)\n    df[\"Date\"] = df[\"Date\"].dt.strftime(format_str) \n    df = df.merge(vix_z_df, on=[\"Date\"], how=\"left\")\n    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n    return df\n\n\ndef add_spread_z(existing_df: pd.DataFrame, buffer_days=380) -> pd.DataFrame:\n    \"\"\"\n    Use existing OHLCV df, pull buffered history, compute young-safe spread_z on the combined\n    Then merge back only the target window rows to prevent nulls.\n    \"\"\"\n    df = existing_df.copy()\n    start, end = df[\"Date\"].min(), df[\"Date\"].max()\n\n    tickers = sorted(df['ticker'].unique())\n    fetch_start = start - timedelta(days=buffer_days)\n    fetch_end   = end\n\n    # You already have get_tickers_history(tickers, start, end)\n    hist = get_tickers_history(tickers, fetch_start, fetch_end)\n    hist[\"Date\"] = pd.to_datetime(hist[\"Date\"], utc=True)\n\n    # Combine buffer + existing; keep existing rows on overlap\n    combined = pd.concat([hist, df], ignore_index=True)\n    combined = combined.sort_values(['ticker', \"Date\"])\n    combined = combined.drop_duplicates(subset=['ticker', \"Date\"], keep=\"last\")\n\n    # Compute young-safe spread_z on the full combined range\n    combined = get_spread_z(combined)\n\n    # Merge only computed columns back to target window\n    cols_to_merge = ['ticker', 'Date', \"spread_z\"]\n    out = df.merge(combined[cols_to_merge], on=['ticker', 'Date'], how=\"left\")\n\n    # Final minimal, causal clean-up to guarantee NON-NULL spread_z in target window:\n    # 1) per-ticker forward-fill (past only), 2) same-day cross-section median, 3) final 0\n    out[\"spread_z\"] = (\n        out.groupby('ticker')[\"spread_z\"].ffill()\n           .fillna(out.groupby('Date')[\"spread_z\"].transform(\"median\"))\n           .fillna(0.0)\n    ).clip(-3, 3)\n\n    return out\n\n\ndef add_adv_dollar(df):\n    adv_df = get_adv_dollar(df)\n    \n    df = df.merge(\n        adv_df,\n        on=[\"Date\", \"ticker\"],\n        how=\"left\",\n    )\n    return df\n\n\ndef count_headlines_all_days(news_df):\n    \"\"\"Counts the number of headlines for each ticker symbol each day\n    Input: news_df pandas dataframe with ticker column for ticker symbols and date for the headline date\n    Output: pandas dataframe containing the number of headlines per ticker per day\n                indexes are dates in string and tickers as the column names\"\"\"\n    \n    #check columns in dataframe\n    columns = list(news_df.columns)\n    if (('date' not in columns) and ('Date' not in columns)) or (('ticker' not in columns) and ('Stock_symbol' not in columns)):\n        print('input dataframe does not have both ticker and date columns')\n        return pd.DataFrame()\n    \n    #find column names\n    date_col = 'date' if 'date' in columns else 'Date'\n    ticker_col = 'ticker' if 'ticker' in columns else 'Stock_symbol'\n    \n    # Count occurrences in the date column\n    headline_dates = news_df[date_col]#.str[:10]#.value_counts()\n    df = pd.DataFrame({ticker_col: news_df[ticker_col],\n                       date_col: headline_dates})\n    \n    # count headlines per day per ticker\n    df = df.groupby([date_col, ticker_col]).size().unstack(fill_value=0)\n    \n    #create list of dates needed\n    format_code = \"%Y-%m-%d\"# Corresponds to 'YYYY-MM-DD'\n    set_of_dates = set(df.index)\n    date_min = start#datetime.strptime(min('2010-01-04',min(set_of_dates)), format_code).date() #datetime(2000,1,1).date()#\n    date_max = end #datetime.strptime(max('2018-12-28',max(set_of_dates)), format_code).date()\n    date_lst = [(date_min+timedelta(i)) for i in range(int((date_max-date_min).days)+1)]\n    \n    #find dates not in dataframe\n    missing_dates = dict([(day,int(0)) for day in set(date_lst).difference(set(df.index))])\n    \n    #add missing dates to dataframe\n    tickers = list(set(df.columns))\n    tickers.sort()\n    empty_dict = dict([(ticker, missing_dates) for ticker in tickers])\n    add_dates = pd.DataFrame(empty_dict)\n    df = pd.concat([df, add_dates], ignore_index=False)\n    \n    #sort rows and columns\n    df = df.sort_index()\n    df = df.T\n    df = df.sort_index()\n    \n    return df\n\n\ndef add_news_flag(news_df, price_df):\n    \"\"\"adds a new news flag column: 0=no news, 1=news\n    input: news_df with 'date', 'ticker', and other columns\n            price_df with 'Date', 'ticker', and other columns\n            optional start and end Timestamps\n    output: dataframe df\n    \"\"\"\n\n    #count headlines per ticker per day\n    news_count = count_headlines_all_days(news_df)\n    \n    #filter count_df by date\n    news_count = news_count.T\n    news_count['date'] = pd.to_datetime(list(news_count.index), utc=True)\n    \n    #convert news_count df to different format\n    news_cols = list(news_count.columns)\n    news_count = news_count.melt(id_vars=['date'], value_vars=news_cols, \n                  var_name='ticker', value_name='news flag')\n\n    # change count to flag: 0=no news, 1=news\n    news_count['news flag'] = [flag if flag < 2 else 1 for flag in news_count['news flag']]\n    news_count['date'] = pd.to_datetime(news_count['date'], utc=True)\n    news_count.sort_values(['date','ticker'], inplace=True)\n    \n    #add news flag: 0=no news, 1=news\n    price_df = pd.merge(price_df, news_count, left_on=['Date','ticker'], \n              right_on=['date','ticker'])\n\n    return price_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T20:41:22.720857Z","iopub.execute_input":"2025-12-07T20:41:22.721619Z","iopub.status.idle":"2025-12-07T20:41:22.741243Z","shell.execute_reply.started":"2025-12-07T20:41:22.721595Z","shell.execute_reply":"2025-12-07T20:41:22.740367Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Data Loading/Sourcing","metadata":{}},{"cell_type":"markdown","source":"## Load News Dataset","metadata":{}},{"cell_type":"code","source":"# Get dataset from Kaggle Hub\nimport kagglehub\ndir_path = kagglehub.dataset_download(\"zeroadamantium/nasdaq-news\", force_download=True)\nfile_name = \"nasdaq_news.csv\"\npath = os.path.join(dir_path, file_name)\nprint(path)\n\n# This will print all the folders in the input directory\nprint(os.listdir(\"/kaggle/input\"))\n\n# Once you see the folder name above, replace 'YOUR_FOLDER_NAME' below to see the files inside\n# print(os.listdir(\"/kaggle/input/YOUR_FOLDER_NAME\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:10:18.064059Z","iopub.execute_input":"2025-12-07T18:10:18.064829Z","iopub.status.idle":"2025-12-07T18:10:18.199793Z","shell.execute_reply.started":"2025-12-07T18:10:18.064800Z","shell.execute_reply":"2025-12-07T18:10:18.199175Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nasdaq-news/nasdaq_news.csv\n['news-trading', 'nasdaq-news']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"filename = \"/kaggle/input/nasdaq-news/nasdaq_news.csv\"\nnews_df = pd.read_csv(filename)\nnews_df = news_df.drop(columns=['Article'])\n\n# 1. Check Local (Git/PC) first\n# if os.path.exists(filename):\n#     news_df = pd.read_csv(filename)\n#     print(f\"Success: Loaded {filename} from local folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:10:18.200815Z","iopub.execute_input":"2025-12-07T18:10:18.201060Z","iopub.status.idle":"2025-12-07T18:12:07.076821Z","shell.execute_reply.started":"2025-12-07T18:10:18.201042Z","shell.execute_reply":"2025-12-07T18:12:07.076116Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"start = pd.Timestamp('2010-01-04 05:00:00+0000', tz='UTC')\nend   = pd.Timestamp('2018-12-28 05:00:00+0000', tz='UTC')\n\n# Limit news to start and stop times\nnews_df['Date'] = pd.to_datetime(list(news_df['Date']), utc=True)\nnews_df = news_df[news_df['Date'] >= start]\nnews_df = news_df[news_df['Date'] <= end]\nprint(news_df.shape)\n\ntickers = tickers_with_most_headlines(news_df, str(start), str(end), 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:12:07.084336Z","iopub.execute_input":"2025-12-07T18:12:07.084567Z","iopub.status.idle":"2025-12-07T18:12:08.834257Z","shell.execute_reply.started":"2025-12-07T18:12:07.084550Z","shell.execute_reply":"2025-12-07T18:12:08.833389Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Source Pricing Dataset (Yahoo Finance)","metadata":{}},{"cell_type":"code","source":"# Get yfinance ticker history for all tickers in tickers df\n# yfinance will produce the \"possibly delisted\" message for tickers without information\ndf = get_tickers_history(list(tickers['ticker']), start=start, end=end)\n\n# Remove unnecessary columns\ndf = df.drop(['Capital Gains','Adj Close'], axis=1)\n\n# Limit df to only 200 tickers and tickers with data\nkeep_tickers = list(df['ticker'].drop_duplicates()[:200])\ndf = df[df['ticker'].isin(keep_tickers)]\ntickers = tickers[tickers['ticker'].isin(keep_tickers)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:12:08.836035Z","iopub.execute_input":"2025-12-07T18:12:08.836282Z","iopub.status.idle":"2025-12-07T18:12:57.918072Z","shell.execute_reply.started":"2025-12-07T18:12:08.836264Z","shell.execute_reply":"2025-12-07T18:12:57.917155Z"}},"outputs":[{"name":"stderr","text":"ERROR:yfinance:$X: possibly delisted; no timezone found\nERROR:yfinance:$DISH: possibly delisted; no timezone found\nERROR:yfinance:$WBA: possibly delisted; no timezone found\nERROR:yfinance:$FL: possibly delisted; no timezone found\nERROR:yfinance:$SPWR: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00) (Yahoo error = \"Data doesn't exist for startDate = 1262581200, endDate = 1546059600\")\nERROR:yfinance:$BRK: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00)\nERROR:yfinance:$DFS: possibly delisted; no timezone found\nERROR:yfinance:$PXD: possibly delisted; no timezone found\nERROR:yfinance:$AI: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00) (Yahoo error = \"Data doesn't exist for startDate = 1262581200, endDate = 1546059600\")\nERROR:yfinance:$MRO: possibly delisted; no timezone found\nERROR:yfinance:$AMTD: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00) (Yahoo error = \"Data doesn't exist for startDate = 1262581200, endDate = 1546059600\")\nERROR:yfinance:$BIG: possibly delisted; no timezone found\nERROR:yfinance:$HES: possibly delisted; no timezone found\nERROR:yfinance:$HA: possibly delisted; no timezone found\nERROR:yfinance:$CONN: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00)\nERROR:yfinance:$SWN: possibly delisted; no timezone found\n/kaggle/working/multimodal-eq-sizing/src/data/loaders.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  return pd.concat(tickers_history_dfs)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Data Preprocessing and Enrichment","metadata":{}},{"cell_type":"markdown","source":"## Add Pricing Features","metadata":{}},{"cell_type":"code","source":"# 1. Add Sector\ndf = add_sector(df)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:12:57.919124Z","iopub.execute_input":"2025-12-07T18:12:57.919366Z","iopub.status.idle":"2025-12-07T18:13:53.894201Z","shell.execute_reply.started":"2025-12-07T18:12:57.919348Z","shell.execute_reply":"2025-12-07T18:13:53.893506Z"}},"outputs":[{"name":"stdout","text":"(391851, 10)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# 2. Add Leg One Indicators\nleg_one_inds = calculate_leg_one_features(df)\nprint(leg_one_inds.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:13:53.894941Z","iopub.execute_input":"2025-12-07T18:13:53.895403Z","iopub.status.idle":"2025-12-07T18:13:54.912708Z","shell.execute_reply.started":"2025-12-07T18:13:53.895383Z","shell.execute_reply":"2025-12-07T18:13:54.911983Z"}},"outputs":[{"name":"stdout","text":"(391851, 19)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 3. Add Excess Return\ndf = add_excess_return(df, start, end)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:13:54.913474Z","iopub.execute_input":"2025-12-07T18:13:54.913665Z","iopub.status.idle":"2025-12-07T18:13:55.260803Z","shell.execute_reply.started":"2025-12-07T18:13:54.913651Z","shell.execute_reply":"2025-12-07T18:13:55.260017Z"}},"outputs":[{"name":"stdout","text":"(391851, 12)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 4. Add VIX_Z\ndf = add_vix_z(df, start, end)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:13:55.261731Z","iopub.execute_input":"2025-12-07T18:13:55.262077Z","iopub.status.idle":"2025-12-07T18:13:57.918549Z","shell.execute_reply.started":"2025-12-07T18:13:55.262057Z","shell.execute_reply":"2025-12-07T18:13:57.917810Z"}},"outputs":[{"name":"stdout","text":"Yay!ðŸ¥³\n(391851, 14)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# 5. Add SPREAD_Z\ndf = add_spread_z(df)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:13:57.919371Z","iopub.execute_input":"2025-12-07T18:13:57.919726Z","iopub.status.idle":"2025-12-07T18:14:30.124295Z","shell.execute_reply.started":"2025-12-07T18:13:57.919703Z","shell.execute_reply":"2025-12-07T18:14:30.123227Z"}},"outputs":[{"name":"stdout","text":"(391851, 15)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# 6. Add ADV_DOLLAR\ndf = add_adv_dollar(df)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:14:30.126605Z","iopub.execute_input":"2025-12-07T18:14:30.126837Z","iopub.status.idle":"2025-12-07T18:14:30.421805Z","shell.execute_reply.started":"2025-12-07T18:14:30.126819Z","shell.execute_reply":"2025-12-07T18:14:30.420999Z"}},"outputs":[{"name":"stdout","text":"(391851, 17)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 7. Preprocessing and Clean up\n\n# Change Date formats to UTCdf['Date'] = pd.to_datetime(df['Date'], utc=True).dt.normalize()\nleg_one_inds['Date'] = pd.to_datetime(leg_one_inds['Date'], utc=True).dt.normalize()\n\n# Select only the new features\njoin_keys = ['ticker', 'Date']\nnew_features = [col for col in leg_one_inds.columns if col not in df.columns or col in join_keys]\nleg_one_clean = leg_one_inds[new_features].copy()\n\n# Remove duplicate rows in the indicators\nleg_one_clean = leg_one_clean.drop_duplicates(subset=join_keys)\n\n# Merge\ndf = df.merge(\n    leg_one_clean, \n    on=join_keys, \n    how='left'\n)\n\n# Verification\nprint(f\"New shape: {df.shape}\")\nprint(\"New columns added:\", list(set(df.columns) - set(leg_one_inds.columns).symmetric_difference(set(new_features))))\n# (Simple print of added columns for sanity check)\nprint(\"Added Columns:\", [c for c in new_features if c not in join_keys])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:14:30.422893Z","iopub.execute_input":"2025-12-07T18:14:30.423195Z","iopub.status.idle":"2025-12-07T18:14:30.663411Z","shell.execute_reply.started":"2025-12-07T18:14:30.423176Z","shell.execute_reply":"2025-12-07T18:14:30.662742Z"}},"outputs":[{"name":"stdout","text":"New shape: (391851, 26)\nNew columns added: ['vol_parkinson_20d', 'dollar_volume', 'VIX_Close', 'r1', 'r5', 'mom_rank', 'spread_z', 'o2c_return', 'VIX_z', 'ticker', 'Date', 'r10', 'excess_return', 'mom_12_1', 'adv_dollar', 'spy_r1', 'trend_ema_diff', 'vol_realized_20d']\nAdded Columns: ['r1', 'r5', 'r10', 'trend_ema_diff', 'vol_realized_20d', 'vol_parkinson_20d', 'mom_12_1', 'mom_rank', 'spy_r1']\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 8. Add NEXT_DAY EXCESS RETURN\ndf[\"next_day_excess_return\"] = df.groupby('ticker')['excess_return'].shift(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:14:30.664148Z","iopub.execute_input":"2025-12-07T18:14:30.664503Z","iopub.status.idle":"2025-12-07T18:14:30.692924Z","shell.execute_reply.started":"2025-12-07T18:14:30.664485Z","shell.execute_reply":"2025-12-07T18:14:30.692013Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 9. Add flags train/val/test flags to identify split sets\ndf['split'] = 'train'\ndf.loc[df['Date'] >=\"2015-01-01\", \"split\"] = \"val\"\ndf.loc[df['Date'] >= \"2017-01-01\", \"split\"] = 'test'\n\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:14:30.693819Z","iopub.execute_input":"2025-12-07T18:14:30.694141Z","iopub.status.idle":"2025-12-07T18:14:30.708762Z","shell.execute_reply.started":"2025-12-07T18:14:30.694122Z","shell.execute_reply":"2025-12-07T18:14:30.708176Z"}},"outputs":[{"name":"stdout","text":"(391851, 28)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# 10. Add NEWS_FLAG\ndf = add_news_flag(news_df, df)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:14:30.709575Z","iopub.execute_input":"2025-12-07T18:14:30.709910Z","iopub.status.idle":"2025-12-07T18:16:14.704782Z","shell.execute_reply.started":"2025-12-07T18:14:30.709883Z","shell.execute_reply":"2025-12-07T18:16:14.704022Z"}},"outputs":[{"name":"stdout","text":"(391686, 30)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:23:30.400433Z","iopub.execute_input":"2025-12-07T18:23:30.401093Z","iopub.status.idle":"2025-12-07T18:23:30.430768Z","shell.execute_reply.started":"2025-12-07T18:23:30.401068Z","shell.execute_reply":"2025-12-07T18:23:30.430207Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                       Date       Open       High        Low      Close  \\\n0 2010-01-05 00:00:00+00:00  53.698801  54.428219  53.664070  53.941940   \n1 2010-01-06 00:00:00+00:00  53.941910  54.254516  53.629303  53.664040   \n2 2010-01-07 00:00:00+00:00  53.768241  57.241645  53.594575  56.442760   \n3 2010-01-08 00:00:00+00:00  56.651173  57.971062  56.512232  57.658455   \n4 2010-01-11 00:00:00+00:00  58.457350  58.631016  57.450062  58.214211   \n\n       Volume  Dividends  Stock Splits ticker       sector  ...  \\\n0  13469263.0        0.0           0.0     GE  Industrials  ...   \n1  11573422.0        0.0           0.0     GE  Industrials  ...   \n2  38701038.0        0.0           0.0     GE  Industrials  ...   \n3  24019636.0        0.0           0.0     GE  Industrials  ...   \n4  15999249.0        0.0           0.0     GE  Industrials  ...   \n\n   trend_ema_diff  vol_realized_20d  vol_parkinson_20d  mom_12_1  mom_rank  \\\n0        0.032598               NaN                NaN       NaN       NaN   \n1        0.024568               NaN                NaN       NaN       NaN   \n2        0.344084               NaN                NaN       NaN       NaN   \n3        0.727206               NaN                NaN       NaN       NaN   \n4        1.077108               NaN                NaN       NaN       NaN   \n\n     spy_r1  next_day_excess_return  split                      date  \\\n0  0.002647               -0.006825  train 2010-01-05 00:00:00+00:00   \n1  0.000704                0.043662  train 2010-01-06 00:00:00+00:00   \n2  0.004221                0.011810  train 2010-01-07 00:00:00+00:00   \n3  0.003328               -0.001118  train 2010-01-08 00:00:00+00:00   \n4  0.001396                0.014180  train 2010-01-11 00:00:00+00:00   \n\n   news flag  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  \n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>...</th>\n      <th>trend_ema_diff</th>\n      <th>vol_realized_20d</th>\n      <th>vol_parkinson_20d</th>\n      <th>mom_12_1</th>\n      <th>mom_rank</th>\n      <th>spy_r1</th>\n      <th>next_day_excess_return</th>\n      <th>split</th>\n      <th>date</th>\n      <th>news flag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-05 00:00:00+00:00</td>\n      <td>53.698801</td>\n      <td>54.428219</td>\n      <td>53.664070</td>\n      <td>53.941940</td>\n      <td>13469263.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.032598</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002647</td>\n      <td>-0.006825</td>\n      <td>train</td>\n      <td>2010-01-05 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-06 00:00:00+00:00</td>\n      <td>53.941910</td>\n      <td>54.254516</td>\n      <td>53.629303</td>\n      <td>53.664040</td>\n      <td>11573422.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.024568</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000704</td>\n      <td>0.043662</td>\n      <td>train</td>\n      <td>2010-01-06 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-07 00:00:00+00:00</td>\n      <td>53.768241</td>\n      <td>57.241645</td>\n      <td>53.594575</td>\n      <td>56.442760</td>\n      <td>38701038.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.344084</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004221</td>\n      <td>0.011810</td>\n      <td>train</td>\n      <td>2010-01-07 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-08 00:00:00+00:00</td>\n      <td>56.651173</td>\n      <td>57.971062</td>\n      <td>56.512232</td>\n      <td>57.658455</td>\n      <td>24019636.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.727206</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.003328</td>\n      <td>-0.001118</td>\n      <td>train</td>\n      <td>2010-01-08 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-11 00:00:00+00:00</td>\n      <td>58.457350</td>\n      <td>58.631016</td>\n      <td>57.450062</td>\n      <td>58.214211</td>\n      <td>15999249.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>1.077108</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.001396</td>\n      <td>0.014180</td>\n      <td>train</td>\n      <td>2010-01-11 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## Checkpoint: Save datasets","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/datasets\nnews_df.to_csv('/kaggle/working/datasets/filtered_news_dataset.csv', index=False)\ntickers.to_csv('/kaggle/working/datasets/top_tickers.csv', index=False)\ndf.to_csv('/kaggle/working/datasets/prices_dataset.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T18:43:29.273229Z","iopub.execute_input":"2025-12-07T18:43:29.273541Z","iopub.status.idle":"2025-12-07T18:43:29.669134Z","shell.execute_reply.started":"2025-12-07T18:43:29.273521Z","shell.execute_reply":"2025-12-07T18:43:29.668290Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import datetime as _dt\nimport kagglehub\n\n# Save files to Kaggle Hub\nhandle = \"zeroadamantium/nasdaq-news\"\nlocal_dataset_dir = \"/kaggle/working/datasets\"\ncurrent_date = _dt.date.today().strftime(\"%Y-%m-%d\")\nkagglehub.dataset_upload(handle, local_dataset_dir, version_notes= f\"Prepare Data Notebook {current_date}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T19:51:59.531601Z","iopub.execute_input":"2025-12-07T19:51:59.531883Z","iopub.status.idle":"2025-12-07T19:52:06.235176Z","shell.execute_reply.started":"2025-12-07T19:51:59.531866Z","shell.execute_reply":"2025-12-07T19:52:06.234399Z"}},"outputs":[{"name":"stdout","text":"Uploading Dataset https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news ...\nStarting upload for file /kaggle/working/datasets/prices_dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199M/199M [00:02<00:00, 97.9MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/prices_dataset.csv (190MB)\nStarting upload for file /kaggle/working/datasets/filtered_news_dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74.7M/74.7M [00:00<00:00, 99.3MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/filtered_news_dataset.csv (71MB)\nStarting upload for file /kaggle/working/datasets/top_tickers.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.78k/1.78k [00:00<00:00, 8.85kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/top_tickers.csv (2KB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Your dataset has been created.\nFiles are being processed...\nSee at: https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"## Add News Features","metadata":{}},{"cell_type":"code","source":"news_features_df = built_news_features(\n    ner_text_column = \"Article_title\",\n    output_path = \"/kaggle/working/datasets/news_features.pkl\",\n    file_path = \"/kaggle/working/datasets/filtered_news_dataset.csv\",\n    chunk_size = 100_000,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T20:42:18.952760Z","iopub.execute_input":"2025-12-07T20:42:18.953118Z","iopub.status.idle":"2025-12-07T21:27:15.920799Z","shell.execute_reply.started":"2025-12-07T20:42:18.953094Z","shell.execute_reply":"2025-12-07T21:27:15.920018Z"}},"outputs":[{"name":"stdout","text":"Loading NER model...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Loading encoder model for z_news (FinBERT)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Loading dataset in chunks...\nFound 1 CSV files:\n  - /kaggle/working/datasets/filtered_news_dataset.csv\nProcessing chunk with 100000 rows...\n","output_type":"stream"},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Encoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 100000 rows...\nEncoding headline embeddings (FinBERT) on 100000 rows...\nRunning NER on column 'Article_title' (batched) for 100000 rows...\nProcessing chunk with 2424 rows...\nEncoding headline embeddings (FinBERT) on 2424 rows...\nRunning NER on column 'Article_title' (batched) for 2424 rows...\nTotal rows processed across chunks: 802424\nAggregating per (Stock_symbol, Date)...\nComputing novelty...\nComputing novelty in parallel for 3692 stocks with 4 workers...\nAggregating embeddings into z_news (recency-weighted pooling)...\n[aggregate_z_news] Initialized attention vector w with dim=768\nSaving features to /kaggle/working/datasets/news_features.pkl ...\nDone.\nColumns in final output:\nIndex(['Date', 'Stock_symbol', 'velocity', 'novelty', 'earnings_flag',\n       'guidance_flag', 'merger_flag', 'rating_flag', 'z_news',\n       'entities_today'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"news_features_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T21:28:03.811751Z","iopub.execute_input":"2025-12-07T21:28:03.812067Z","iopub.status.idle":"2025-12-07T21:28:03.982281Z","shell.execute_reply.started":"2025-12-07T21:28:03.812042Z","shell.execute_reply":"2025-12-07T21:28:03.981603Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(549379, 10)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Checkpoint: Save News Features Dataset","metadata":{}},{"cell_type":"code","source":"news_features_df.to_csv(\"/kaggle/working/datasets/news_features.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T21:31:28.035167Z","iopub.execute_input":"2025-12-07T21:31:28.035526Z","iopub.status.idle":"2025-12-07T22:12:41.365420Z","shell.execute_reply.started":"2025-12-07T21:31:28.035507Z","shell.execute_reply":"2025-12-07T22:12:41.364792Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import datetime as _dt\nimport kagglehub\n\n# Save files to Kaggle Hub\nhandle = \"zeroadamantium/nasdaq-news\"\nlocal_dataset_dir = \"/kaggle/working/datasets\"\ncurrent_date = _dt.date.today().strftime(\"%Y-%m-%d\")\nkagglehub.dataset_upload(handle, local_dataset_dir, version_notes= f\"Added News Features {current_date}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:14:09.065333Z","iopub.execute_input":"2025-12-07T22:14:09.066203Z","iopub.status.idle":"2025-12-07T22:15:35.176336Z","shell.execute_reply.started":"2025-12-07T22:14:09.066177Z","shell.execute_reply":"2025-12-07T22:15:35.175532Z"}},"outputs":[{"name":"stdout","text":"Uploading Dataset https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news ...\nStarting upload for file /kaggle/working/datasets/news_features.csv\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.90G/6.90G [01:09<00:00, 98.9MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/news_features.csv (6GB)\nStarting upload for file /kaggle/working/datasets/prices_dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199M/199M [00:02<00:00, 98.4MB/s] ","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/prices_dataset.csv (190MB)\nStarting upload for file /kaggle/working/datasets/filtered_news_dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.5M/75.5M [00:01<00:00, 71.6MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/filtered_news_dataset.csv (72MB)\nStarting upload for file /kaggle/working/datasets/top_tickers.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.78k/1.78k [00:00<00:00, 8.57kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/top_tickers.csv (2KB)\nStarting upload for file /kaggle/working/datasets/news_features.pkl\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.76G/1.76G [00:07<00:00, 228MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/news_features.pkl (2GB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Your dataset has been created.\nFiles are being processed...\nSee at: https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(news_features_df.shape)\nnews_features_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:30:21.751288Z","iopub.execute_input":"2025-12-07T22:30:21.751910Z","iopub.status.idle":"2025-12-07T22:30:21.768915Z","shell.execute_reply.started":"2025-12-07T22:30:21.751888Z","shell.execute_reply":"2025-12-07T22:30:21.768010Z"}},"outputs":[{"name":"stdout","text":"(549379, 10)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                       Date Stock_symbol  velocity  novelty  earnings_flag  \\\n0 2010-01-12 00:00:00+00:00           AA         1      0.0              0   \n1 2010-01-18 00:00:00+00:00           AA         1      1.0              0   \n2 2010-02-03 00:00:00+00:00           AA         1      0.0              0   \n3 2010-02-16 00:00:00+00:00           AA         1      1.0              0   \n4 2010-03-02 00:00:00+00:00           AA         1      1.0              0   \n\n   guidance_flag  merger_flag  rating_flag  \\\n0              0            0            0   \n1              0            0            0   \n2              0            0            0   \n3              0            0            0   \n4              0            0            0   \n\n                                              z_news       entities_today  \n0  [-0.09942879, -0.7900245, -1.5315706, -0.03880...                   []  \n1  [-0.12355294, -0.22699751, -0.54240745, -0.222...      [ci, group, ti]  \n2  [-0.31843555, -0.499312, -1.284549, 0.3205093,...                   []  \n3  [-0.32647607, -0.513768, -0.8571328, 1.0574433...              [bulls]  \n4  [0.57808304, -0.20705728, -1.0697136, 0.525321...  [citigroup, pandit]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Stock_symbol</th>\n      <th>velocity</th>\n      <th>novelty</th>\n      <th>earnings_flag</th>\n      <th>guidance_flag</th>\n      <th>merger_flag</th>\n      <th>rating_flag</th>\n      <th>z_news</th>\n      <th>entities_today</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-12 00:00:00+00:00</td>\n      <td>AA</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-0.09942879, -0.7900245, -1.5315706, -0.03880...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-18 00:00:00+00:00</td>\n      <td>AA</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-0.12355294, -0.22699751, -0.54240745, -0.222...</td>\n      <td>[ci, group, ti]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-02-03 00:00:00+00:00</td>\n      <td>AA</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-0.31843555, -0.499312, -1.284549, 0.3205093,...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-02-16 00:00:00+00:00</td>\n      <td>AA</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[-0.32647607, -0.513768, -0.8571328, 1.0574433...</td>\n      <td>[bulls]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-03-02 00:00:00+00:00</td>\n      <td>AA</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0.57808304, -0.20705728, -1.0697136, 0.525321...</td>\n      <td>[citigroup, pandit]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Checkpoint: Load Prices Dataset","metadata":{}},{"cell_type":"code","source":"prices_dataset_df = pd.read_csv(\"/kaggle/working/datasets/prices_dataset.csv\")\nprices_dataset_df['Date'] = pd.to_datetime(prices_dataset_df['Date'], utc=True)\nprint(prices_dataset_df.shape)\nprices_dataset_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:32:22.197702Z","iopub.execute_input":"2025-12-07T22:32:22.198067Z","iopub.status.idle":"2025-12-07T22:32:22.220886Z","shell.execute_reply.started":"2025-12-07T22:32:22.198043Z","shell.execute_reply":"2025-12-07T22:32:22.220240Z"}},"outputs":[{"name":"stdout","text":"(391686, 30)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                        Date       Open       High        Low      Close  \\\n0  2010-01-05 00:00:00+00:00  53.698801  54.428219  53.664070  53.941940   \n1  2010-01-06 00:00:00+00:00  53.941910  54.254516  53.629303  53.664040   \n2  2010-01-07 00:00:00+00:00  53.768241  57.241645  53.594575  56.442760   \n3  2010-01-08 00:00:00+00:00  56.651173  57.971062  56.512232  57.658455   \n4  2010-01-11 00:00:00+00:00  58.457350  58.631016  57.450062  58.214211   \n\n       Volume  Dividends  Stock Splits ticker       sector  ...  \\\n0  13469263.0        0.0           0.0     GE  Industrials  ...   \n1  11573422.0        0.0           0.0     GE  Industrials  ...   \n2  38701038.0        0.0           0.0     GE  Industrials  ...   \n3  24019636.0        0.0           0.0     GE  Industrials  ...   \n4  15999249.0        0.0           0.0     GE  Industrials  ...   \n\n   trend_ema_diff  vol_realized_20d  vol_parkinson_20d  mom_12_1  mom_rank  \\\n0        0.032598               NaN                NaN       NaN       NaN   \n1        0.024568               NaN                NaN       NaN       NaN   \n2        0.344084               NaN                NaN       NaN       NaN   \n3        0.727206               NaN                NaN       NaN       NaN   \n4        1.077108               NaN                NaN       NaN       NaN   \n\n     spy_r1  next_day_excess_return  split                       date  \\\n0  0.002647               -0.006825  train  2010-01-05 00:00:00+00:00   \n1  0.000704                0.043662  train  2010-01-06 00:00:00+00:00   \n2  0.004221                0.011810  train  2010-01-07 00:00:00+00:00   \n3  0.003328               -0.001118  train  2010-01-08 00:00:00+00:00   \n4  0.001396                0.014180  train  2010-01-11 00:00:00+00:00   \n\n   news flag  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  \n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>...</th>\n      <th>trend_ema_diff</th>\n      <th>vol_realized_20d</th>\n      <th>vol_parkinson_20d</th>\n      <th>mom_12_1</th>\n      <th>mom_rank</th>\n      <th>spy_r1</th>\n      <th>next_day_excess_return</th>\n      <th>split</th>\n      <th>date</th>\n      <th>news flag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-05 00:00:00+00:00</td>\n      <td>53.698801</td>\n      <td>54.428219</td>\n      <td>53.664070</td>\n      <td>53.941940</td>\n      <td>13469263.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.032598</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002647</td>\n      <td>-0.006825</td>\n      <td>train</td>\n      <td>2010-01-05 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-06 00:00:00+00:00</td>\n      <td>53.941910</td>\n      <td>54.254516</td>\n      <td>53.629303</td>\n      <td>53.664040</td>\n      <td>11573422.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.024568</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000704</td>\n      <td>0.043662</td>\n      <td>train</td>\n      <td>2010-01-06 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-07 00:00:00+00:00</td>\n      <td>53.768241</td>\n      <td>57.241645</td>\n      <td>53.594575</td>\n      <td>56.442760</td>\n      <td>38701038.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.344084</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.004221</td>\n      <td>0.011810</td>\n      <td>train</td>\n      <td>2010-01-07 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010-01-08 00:00:00+00:00</td>\n      <td>56.651173</td>\n      <td>57.971062</td>\n      <td>56.512232</td>\n      <td>57.658455</td>\n      <td>24019636.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>0.727206</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.003328</td>\n      <td>-0.001118</td>\n      <td>train</td>\n      <td>2010-01-08 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010-01-11 00:00:00+00:00</td>\n      <td>58.457350</td>\n      <td>58.631016</td>\n      <td>57.450062</td>\n      <td>58.214211</td>\n      <td>15999249.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>GE</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>1.077108</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.001396</td>\n      <td>0.014180</td>\n      <td>train</td>\n      <td>2010-01-11 00:00:00+00:00</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 30 columns</p>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Final Dataset: Merge Prices and News Features Datasets","metadata":{}},{"cell_type":"code","source":"processed_multimodal_eq_sizing_dataset_df = pd.merge(\n    prices_dataset_df,\n    news_features_df,\n    left_on = ['Date', 'ticker'],\n    right_on = ['Date', 'Stock_symbol'],\n    how = 'left'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:37:39.649294Z","iopub.execute_input":"2025-12-07T22:37:39.649893Z","iopub.status.idle":"2025-12-07T22:37:40.106856Z","shell.execute_reply.started":"2025-12-07T22:37:39.649870Z","shell.execute_reply":"2025-12-07T22:37:40.106236Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"processed_multimodal_eq_sizing_dataset_df = processed_multimodal_eq_sizing_dataset_df.drop(columns=['Stock_symbol'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:50:55.535248Z","iopub.execute_input":"2025-12-07T22:50:55.536032Z","iopub.status.idle":"2025-12-07T22:50:55.660754Z","shell.execute_reply.started":"2025-12-07T22:50:55.535993Z","shell.execute_reply":"2025-12-07T22:50:55.659881Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"print(processed_multimodal_eq_sizing_dataset_df.shape)\nprocessed_multimodal_eq_sizing_dataset_df.sample(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:51:23.846440Z","iopub.execute_input":"2025-12-07T22:51:23.846716Z","iopub.status.idle":"2025-12-07T22:51:23.880048Z","shell.execute_reply.started":"2025-12-07T22:51:23.846696Z","shell.execute_reply":"2025-12-07T22:51:23.879341Z"}},"outputs":[{"name":"stdout","text":"(391686, 38)\n","output_type":"stream"},{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"                            Date       Open       High        Low      Close  \\\n79298  2018-03-21 00:00:00+00:00  14.012327  14.277348  13.955939  14.187128   \n237209 2014-09-25 00:00:00+00:00   8.955884   8.958680   8.841244   8.885982   \n296435 2016-08-03 00:00:00+00:00   6.379677   6.616366   6.277718   6.503483   \n253505 2016-07-27 00:00:00+00:00  26.793909  27.254408  25.433670  25.596615   \n211375 2018-02-05 00:00:00+00:00  89.849939  91.084050  86.147615  86.276169   \n\n            Volume  Dividends  Stock Splits ticker           sector  ...  \\\n79298    6282800.0        0.0           0.0    EPD           Energy  ...   \n237209  17915100.0        0.0           0.0    CSX      Industrials  ...   \n296435   1983400.0        0.0           0.0     CC  Basic Materials  ...   \n253505  11405300.0        0.0           0.0    DVN           Energy  ...   \n211375    939500.0        0.0           0.0     CE  Basic Materials  ...   \n\n                             date  news flag  velocity  novelty  \\\n79298   2018-03-21 00:00:00+00:00          1       1.0      1.0   \n237209  2014-09-25 00:00:00+00:00          1       1.0      0.5   \n296435  2016-08-03 00:00:00+00:00          0       NaN      NaN   \n253505  2016-07-27 00:00:00+00:00          1       2.0      1.0   \n211375  2018-02-05 00:00:00+00:00          1       1.0      0.4   \n\n        earnings_flag  guidance_flag  merger_flag  rating_flag  \\\n79298             0.0            0.0          0.0          0.0   \n237209            1.0            0.0          0.0          0.0   \n296435            NaN            NaN          NaN          NaN   \n253505            1.0            0.0          0.0          0.0   \n211375            0.0            0.0          1.0          0.0   \n\n                                                   z_news  \\\n79298   [0.22823562, -0.042468984, -0.74258715, -0.145...   \n237209  [-0.21329702, -0.78418994, 0.33254468, -0.8496...   \n296435                                                NaN   \n253505  [-0.09879635, 0.087337136, 0.23949988, -0.8212...   \n211375  [0.21518224, -0.36966068, -1.3591042, -0.10072...   \n\n                                    entities_today  \n79298                   [e, mp, ra energy, se, sr]  \n237209                                     [cs, x]  \n296435                                         NaN  \n253505  [ana, ap, c, d, dar, devon energy, ko, vn]  \n211375     [ce, lane, ni plastics l. l. c, om, se]  \n\n[5 rows x 38 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Dividends</th>\n      <th>Stock Splits</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>...</th>\n      <th>date</th>\n      <th>news flag</th>\n      <th>velocity</th>\n      <th>novelty</th>\n      <th>earnings_flag</th>\n      <th>guidance_flag</th>\n      <th>merger_flag</th>\n      <th>rating_flag</th>\n      <th>z_news</th>\n      <th>entities_today</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>79298</th>\n      <td>2018-03-21 00:00:00+00:00</td>\n      <td>14.012327</td>\n      <td>14.277348</td>\n      <td>13.955939</td>\n      <td>14.187128</td>\n      <td>6282800.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>EPD</td>\n      <td>Energy</td>\n      <td>...</td>\n      <td>2018-03-21 00:00:00+00:00</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[0.22823562, -0.042468984, -0.74258715, -0.145...</td>\n      <td>[e, mp, ra energy, se, sr]</td>\n    </tr>\n    <tr>\n      <th>237209</th>\n      <td>2014-09-25 00:00:00+00:00</td>\n      <td>8.955884</td>\n      <td>8.958680</td>\n      <td>8.841244</td>\n      <td>8.885982</td>\n      <td>17915100.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>CSX</td>\n      <td>Industrials</td>\n      <td>...</td>\n      <td>2014-09-25 00:00:00+00:00</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[-0.21329702, -0.78418994, 0.33254468, -0.8496...</td>\n      <td>[cs, x]</td>\n    </tr>\n    <tr>\n      <th>296435</th>\n      <td>2016-08-03 00:00:00+00:00</td>\n      <td>6.379677</td>\n      <td>6.616366</td>\n      <td>6.277718</td>\n      <td>6.503483</td>\n      <td>1983400.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>CC</td>\n      <td>Basic Materials</td>\n      <td>...</td>\n      <td>2016-08-03 00:00:00+00:00</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>253505</th>\n      <td>2016-07-27 00:00:00+00:00</td>\n      <td>26.793909</td>\n      <td>27.254408</td>\n      <td>25.433670</td>\n      <td>25.596615</td>\n      <td>11405300.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>DVN</td>\n      <td>Energy</td>\n      <td>...</td>\n      <td>2016-07-27 00:00:00+00:00</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>[-0.09879635, 0.087337136, 0.23949988, -0.8212...</td>\n      <td>[ana, ap, c, d, dar, devon energy, ko, vn]</td>\n    </tr>\n    <tr>\n      <th>211375</th>\n      <td>2018-02-05 00:00:00+00:00</td>\n      <td>89.849939</td>\n      <td>91.084050</td>\n      <td>86.147615</td>\n      <td>86.276169</td>\n      <td>939500.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>CE</td>\n      <td>Basic Materials</td>\n      <td>...</td>\n      <td>2018-02-05 00:00:00+00:00</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>[0.21518224, -0.36966068, -1.3591042, -0.10072...</td>\n      <td>[ce, lane, ni plastics l. l. c, om, se]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 38 columns</p>\n</div>"},"metadata":{}}],"execution_count":68},{"cell_type":"markdown","source":"## Chekpoint: Save Final Multimodal Eq Sizing Dataset","metadata":{}},{"cell_type":"code","source":"processed_multimodal_eq_sizing_dataset_df.to_pickle(\"/kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:54:51.995393Z","iopub.execute_input":"2025-12-07T22:54:51.996143Z","iopub.status.idle":"2025-12-07T22:54:54.025372Z","shell.execute_reply.started":"2025-12-07T22:54:51.996116Z","shell.execute_reply":"2025-12-07T22:54:54.024546Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"# processed_multimodal_eq_sizing_dataset_df.to_csv(\"/kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:54:54.027256Z","iopub.execute_input":"2025-12-07T22:54:54.027515Z","iopub.status.idle":"2025-12-07T22:54:54.030911Z","shell.execute_reply.started":"2025-12-07T22:54:54.027498Z","shell.execute_reply":"2025-12-07T22:54:54.030163Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"import datetime as _dt\nimport kagglehub\n\n# Save files to Kaggle Hub\nhandle = \"zeroadamantium/nasdaq-news\"\nlocal_dataset_dir = \"/kaggle/working/datasets\"\ncurrent_date = _dt.date.today().strftime(\"%Y-%m-%d\")\nkagglehub.dataset_upload(handle, local_dataset_dir, version_notes= f\"Final Dataset {current_date}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:54:54.031629Z","iopub.execute_input":"2025-12-07T22:54:54.032033Z","iopub.status.idle":"2025-12-07T22:56:35.563768Z","shell.execute_reply.started":"2025-12-07T22:54:54.032010Z","shell.execute_reply":"2025-12-07T22:56:35.563226Z"}},"outputs":[{"name":"stdout","text":"Uploading Dataset https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news ...\nStarting upload for file /kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.pkl\n","output_type":"stream"},{"name":"stderr","text":"Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550M/550M [00:02<00:00, 185MB/s] ","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.pkl (525MB)\nStarting upload for file /kaggle/working/datasets/news_features.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.90G/6.90G [01:21<00:00, 84.3MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/news_features.csv (6GB)\nStarting upload for file /kaggle/working/datasets/prices_dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199M/199M [00:01<00:00, 110MB/s] ","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/prices_dataset.csv (190MB)\nStarting upload for file /kaggle/working/datasets/filtered_news_dataset.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.5M/75.5M [00:00<00:00, 93.5MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/filtered_news_dataset.csv (72MB)\nStarting upload for file /kaggle/working/datasets/top_tickers.csv\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.78k/1.78k [00:00<00:00, 8.86kB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/top_tickers.csv (2KB)\nStarting upload for file /kaggle/working/datasets/news_features.pkl\n","output_type":"stream"},{"name":"stderr","text":"\nUploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.76G/1.76G [00:07<00:00, 222MB/s]","output_type":"stream"},{"name":"stdout","text":"Upload successful: /kaggle/working/datasets/news_features.pkl (2GB)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Your dataset has been created.\nFiles are being processed...\nSee at: https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# get_return_data(\"/kaggle/working/final_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T22:56:35.566971Z","iopub.execute_input":"2025-12-07T22:56:35.567154Z","iopub.status.idle":"2025-12-07T22:56:35.570535Z","shell.execute_reply.started":"2025-12-07T22:56:35.567140Z","shell.execute_reply":"2025-12-07T22:56:35.569970Z"}},"outputs":[],"execution_count":72}]}
