{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13965676,"sourceType":"datasetVersion","datasetId":8902694}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\n# --- CONFIGURATION ---\nGITHUB_EMAIL = \"brianrp09232000@gmail.com\"  # Replace with your actual git email if different\nGITHUB_USERNAME = \"brianrp09232000\"\nREPO_NAME = \"multimodal-eq-sizing\"\nBRANCH_NAME = \"feature/ticket-16-leg-1\"\n# ---------------------\n\n# 1. Retrieve the token securely from Kaggle Secrets\ntry:\n    user_secrets = UserSecretsClient()\n    pat = user_secrets.get_secret(\"github_token\")\n    print(\"âœ… Token retrieved successfully.\")\nexcept Exception as e:\n    print(\"âŒ Error: Could not retrieve 'github_token' from Secrets.\")\n    print(\"Please go to Add-ons -> Secrets and add your token with the label 'github_token'.\")\n    raise e\n\n# 2. Setup paths\nworking_dir = \"/kaggle/working\"\nrepo_path = os.path.join(working_dir, REPO_NAME)\n\n# 3. Clean up previous runs (if any)\nif os.path.exists(repo_path):\n    print(f\"ðŸ§¹ Removing existing repo at {repo_path}...\")\n    !rm -rf {repo_path}\n\n# 4. Clone the repository using the PAT\nprint(f\"â¬‡ï¸ Cloning {REPO_NAME}...\")\nrepo_url = f\"https://{GITHUB_USERNAME}:{pat}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n!git clone {repo_url} {repo_path}\n\n# 5. Checkout the specific branch\nprint(f\"twisted_rightwards_arrows Switching to branch: {BRANCH_NAME}...\")\n%cd {repo_path}\n!git fetch origin {BRANCH_NAME}\n!git checkout {BRANCH_NAME}\n\n# 6. Install dependencies\nprint(\"ðŸ“¦ Installing requirements...\")\n!pip install -r requirements.txt\n\n# 7. Configure Git (needed for the push step later)\n!git config --global user.email \"{GITHUB_EMAIL}\"\n!git config --global user.name \"{GITHUB_USERNAME}\"\n\n# Return to working directory\n%cd {working_dir}\nprint(\"âœ… Setup complete. You are now on branch:\", BRANCH_NAME)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:34:51.930675Z","iopub.execute_input":"2025-12-05T08:34:51.930961Z","iopub.status.idle":"2025-12-05T08:35:04.838653Z","shell.execute_reply.started":"2025-12-05T08:34:51.930940Z","shell.execute_reply":"2025-12-05T08:35:04.837766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Remove the specific Kaggle path. \".\" means \"current directory\"\n!rm -rf multimodal-eq-sizing\n!git clone https://github.com/brianrp09232000/multimodal-eq-sizing.git\n!pip install -r multimodal-eq-sizing/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:35:04.840575Z","iopub.execute_input":"2025-12-05T08:35:04.840864Z","iopub.status.idle":"2025-12-05T08:36:26.926379Z","shell.execute_reply.started":"2025-12-05T08:35:04.840818Z","shell.execute_reply":"2025-12-05T08:36:26.925258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport pathlib\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:36:26.927649Z","iopub.execute_input":"2025-12-05T08:36:26.928027Z","iopub.status.idle":"2025-12-05T08:36:27.258831Z","shell.execute_reply.started":"2025-12-05T08:36:26.927987Z","shell.execute_reply":"2025-12-05T08:36:27.257599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.seterr(invalid=\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:36:27.260073Z","iopub.execute_input":"2025-12-05T08:36:27.260667Z","iopub.status.idle":"2025-12-05T08:36:27.270267Z","shell.execute_reply.started":"2025-12-05T08:36:27.260620Z","shell.execute_reply":"2025-12-05T08:36:27.269417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Uses the current directory where the notebook is running\nrepo_root = pathlib.Path(\"multimodal-eq-sizing\")\nsys.path.append(str(repo_root.resolve())) # .resolve() gets the full absolute path locally","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:36:27.271151Z","iopub.execute_input":"2025-12-05T08:36:27.271497Z","iopub.status.idle":"2025-12-05T08:36:28.305734Z","shell.execute_reply.started":"2025-12-05T08:36:27.271466Z","shell.execute_reply":"2025-12-05T08:36:28.304800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from src.data.loaders import (\n    get_tickers_history,\n    get_return_data,\n    get_excess_return,\n    get_vix_data,\n    get_spread_z,\n    get_sector_map,\n    get_adv_dollar\n)\n\nfrom src.data.features.price_features import calculate_leg_one_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:36:28.306762Z","iopub.execute_input":"2025-12-05T08:36:28.307099Z","iopub.status.idle":"2025-12-05T08:36:40.412050Z","shell.execute_reply.started":"2025-12-05T08:36:28.307070Z","shell.execute_reply":"2025-12-05T08:36:40.411222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from src.data.universe import tickers_with_most_headlines","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:36:40.415105Z","iopub.execute_input":"2025-12-05T08:36:40.415575Z","iopub.status.idle":"2025-12-05T08:37:03.412913Z","shell.execute_reply.started":"2025-12-05T08:36:40.415553Z","shell.execute_reply":"2025-12-05T08:37:03.412019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# This will print all the folders in the input directory\nprint(os.listdir(\"/kaggle/input\"))\n\n# Once you see the folder name above, replace 'YOUR_FOLDER_NAME' below to see the files inside\n# print(os.listdir(\"/kaggle/input/YOUR_FOLDER_NAME\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:37:03.414082Z","iopub.execute_input":"2025-12-05T08:37:03.414959Z","iopub.status.idle":"2025-12-05T08:37:03.420826Z","shell.execute_reply.started":"2025-12-05T08:37:03.414923Z","shell.execute_reply":"2025-12-05T08:37:03.419901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filename = \"/kaggle/input/nasdaq-news/nasdaq_news.csv\"\n\nnews_df = pd.read_csv(filename)\n\n# 1. Check Local (Git/PC) first\nif os.path.exists(filename):\n    news_df = pd.read_csv(filename)\n    print(f\"Success: Loaded {filename} from local folder.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:37:03.421937Z","iopub.execute_input":"2025-12-05T08:37:03.422330Z","iopub.status.idle":"2025-12-05T08:43:29.930176Z","shell.execute_reply.started":"2025-12-05T08:37:03.422302Z","shell.execute_reply":"2025-12-05T08:43:29.927711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def count_headlines_per_ticker(news_df, start=None, end=None):\n    \"\"\"Counts the number of headlines for each ticker symbol \n    Input: news_df pandas dataframe with ticker column for ticker symbols\n    Output: pandas dataframe containing two columns: ticker names and the \n                number of headlines for the ticker\"\"\"\n    \n    #check columns in dataframe\n    columns = list(news_df.columns)\n    if (('date' not in columns) and ('Date' not in columns)) or (('ticker' not in columns) and ('Stock_symbol' not in columns)):\n        print('input dataframe does not have both ticker and date columns')\n        return pd.DataFrame()\n    \n    #find column names\n    date_col = 'date' if 'date' in columns else 'Date'\n    ticker_col = 'ticker' if 'ticker' in columns else 'Stock_symbol'\n\n    #filter dates\n    if start is not None: \n        start_filter = news_df[date_col] >= str(start)\n        news_df = news_df[start_filter]\n    if end is not None: \n        end_filter = news_df[date_col] <= str(end)\n        news_df = news_df[end_filter]\n    \n    # Count occurrences in a specific column\n    headline_counts = news_df[ticker_col].value_counts()\n    df = headline_counts.to_frame(name='count')\n    df['ticker'] = list(df.index)\n    df = df.reset_index(drop=True)\n    \n    return df[['ticker','count']]\n\n\n\ndef tickers_with_most_headlines(news_df, start=None, end=None, n=200):\n    \"\"\"Finds the tickers with the most headlines \n    Input: news_df pandas dataframe with ticker column for ticker symbols\n            optional: n interger, number of top tickers to return\n    Output: list containing the number of headlines per ticker\n                for the tickers with the most headlines\"\"\"\n    \n    #count headlines for each ticker\n    df = count_headlines_per_ticker(news_df, start, end)\n\n    #limit dataframe to n tickers\n    df = df.sort_values(['count'], ascending=False)\n    df = df[:n]\n    df.reset_index(drop=True, inplace=True)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:43:30.380829Z","iopub.execute_input":"2025-12-05T08:43:30.381880Z","iopub.status.idle":"2025-12-05T08:43:30.410571Z","shell.execute_reply.started":"2025-12-05T08:43:30.381834Z","shell.execute_reply":"2025-12-05T08:43:30.409554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start = pd.Timestamp('2010-01-04 05:00:00+0000', tz='UTC')\nend   = pd.Timestamp('2018-12-28 05:00:00+0000', tz='UTC')\n\n#limit news to start and stop times\nnews_df['Date'] = pd.to_datetime(list(news_df['Date']), utc=True)\nnews_df = news_df[news_df['Date'] >= start]\nnews_df = news_df[news_df['Date'] <= end]\n\ntickers = tickers_with_most_headlines(news_df, str(start), str(end), 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:43:30.412830Z","iopub.execute_input":"2025-12-05T08:43:30.413085Z","iopub.status.idle":"2025-12-05T08:43:34.434393Z","shell.execute_reply.started":"2025-12-05T08:43:30.413067Z","shell.execute_reply":"2025-12-05T08:43:34.433242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#get yfinance ticker history for all tickers in tickers df\n#yfinance will produce the \"possibly delisted\" message for tickers without information\ndf = get_tickers_history(list(tickers['ticker']), start=start, end=end)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:43:34.436827Z","iopub.execute_input":"2025-12-05T08:43:34.437073Z","iopub.status.idle":"2025-12-05T08:45:42.751398Z","shell.execute_reply.started":"2025-12-05T08:43:34.437056Z","shell.execute_reply":"2025-12-05T08:45:42.750412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#remove unnecessary columns\ndf = df.drop(['Capital Gains','Adj Close'], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:42.777297Z","iopub.execute_input":"2025-12-05T08:45:42.777618Z","iopub.status.idle":"2025-12-05T08:45:42.811578Z","shell.execute_reply.started":"2025-12-05T08:45:42.777595Z","shell.execute_reply":"2025-12-05T08:45:42.810619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#limit df to only 200 tickers and tickers with data\nkeep_tickers = list(df['ticker'].drop_duplicates()[:200])\ndf = df[df['ticker'].isin(keep_tickers)]\ntickers = tickers[tickers['ticker'].isin(keep_tickers)]\n\nleg_one_inds = calculate_leg_one_features(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:42.813499Z","iopub.execute_input":"2025-12-05T08:45:42.813797Z","iopub.status.idle":"2025-12-05T08:45:44.524708Z","shell.execute_reply.started":"2025-12-05T08:45:42.813777Z","shell.execute_reply":"2025-12-05T08:45:44.523825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_date_range(df: pd.DataFrame) -> tuple:\n    grouped_by_date = df.groupby([\"ticker\"]).agg(['min', 'max', 'count'])[\"Date\"]\n    start = grouped_by_date[\"min\"].min()\n    end = grouped_by_date[\"max\"].max()\n    return start, end","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:44.527253Z","iopub.execute_input":"2025-12-05T08:45:44.527504Z","iopub.status.idle":"2025-12-05T08:45:44.533099Z","shell.execute_reply.started":"2025-12-05T08:45:44.527484Z","shell.execute_reply":"2025-12-05T08:45:44.531909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_excess_return(df, start, end):\n    excess_return_df = get_excess_return(df, start, end)\n    df = df.merge(excess_return_df, on=[\"ticker\", \"Date\"], how=\"left\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:44.534254Z","iopub.execute_input":"2025-12-05T08:45:44.534696Z","iopub.status.idle":"2025-12-05T08:45:44.546239Z","shell.execute_reply.started":"2025-12-05T08:45:44.534663Z","shell.execute_reply":"2025-12-05T08:45:44.545361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = add_excess_return(df, start, end)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:44.547368Z","iopub.execute_input":"2025-12-05T08:45:44.547652Z","iopub.status.idle":"2025-12-05T08:45:45.114767Z","shell.execute_reply.started":"2025-12-05T08:45:44.547633Z","shell.execute_reply":"2025-12-05T08:45:45.113927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_vix_z(df, start, end):\n    vix_z_df = get_vix_data(start, end)\n    format_str = \"%Y-%m-%d\"\n    vix_z_df[\"Date\"] = vix_z_df[\"Date\"].dt.strftime(format_str)\n    df[\"Date\"] = df[\"Date\"].dt.strftime(format_str) \n    df = df.merge(vix_z_df, on=[\"Date\"], how=\"left\")\n    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:45.116733Z","iopub.execute_input":"2025-12-05T08:45:45.117037Z","iopub.status.idle":"2025-12-05T08:45:45.122746Z","shell.execute_reply.started":"2025-12-05T08:45:45.117016Z","shell.execute_reply":"2025-12-05T08:45:45.121927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = add_vix_z(df, start, end)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:45.123728Z","iopub.execute_input":"2025-12-05T08:45:45.124056Z","iopub.status.idle":"2025-12-05T08:45:48.622548Z","shell.execute_reply.started":"2025-12-05T08:45:45.124036Z","shell.execute_reply":"2025-12-05T08:45:48.621455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_spread_z(existing_df: pd.DataFrame, buffer_days=380) -> pd.DataFrame:\n    \"\"\"\n    Use existing OHLCV df, pull buffered history, compute young-safe spread_z on the combined\n    Then merge back only the target window rows to prevent nulls.\n    \"\"\"\n    df = existing_df.copy()\n    start, end = df[\"Date\"].min(), df[\"Date\"].max()\n\n    tickers = sorted(df['ticker'].unique())\n    fetch_start = start - timedelta(days=buffer_days)\n    fetch_end   = end\n\n    # You already have get_tickers_history(tickers, start, end)\n    hist = get_tickers_history(tickers, fetch_start, fetch_end)\n    hist[\"Date\"] = pd.to_datetime(hist[\"Date\"], utc=True)\n\n    # Combine buffer + existing; keep existing rows on overlap\n    combined = pd.concat([hist, df], ignore_index=True)\n    combined = combined.sort_values(['ticker', \"Date\"])\n    combined = combined.drop_duplicates(subset=['ticker', \"Date\"], keep=\"last\")\n\n    # Compute young-safe spread_z on the full combined range\n    combined = get_spread_z(combined)\n\n    # Merge only computed columns back to target window\n    cols_to_merge = ['ticker', 'Date', \"spread_z\"]\n    out = df.merge(combined[cols_to_merge], on=['ticker', 'Date'], how=\"left\")\n\n    # Final minimal, causal clean-up to guarantee NON-NULL spread_z in target window:\n    # 1) per-ticker forward-fill (past only), 2) same-day cross-section median, 3) final 0\n    out[\"spread_z\"] = (\n        out.groupby('ticker')[\"spread_z\"].ffill()\n           .fillna(out.groupby('Date')[\"spread_z\"].transform(\"median\"))\n           .fillna(0.0)\n    ).clip(-3, 3)\n\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:48.624284Z","iopub.execute_input":"2025-12-05T08:45:48.624611Z","iopub.status.idle":"2025-12-05T08:45:48.633062Z","shell.execute_reply.started":"2025-12-05T08:45:48.624585Z","shell.execute_reply":"2025-12-05T08:45:48.632213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = add_spread_z(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:45:48.634081Z","iopub.execute_input":"2025-12-05T08:45:48.634763Z","iopub.status.idle":"2025-12-05T08:46:45.199562Z","shell.execute_reply.started":"2025-12-05T08:45:48.634743Z","shell.execute_reply":"2025-12-05T08:46:45.198725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_sector(df):\n    tickers = df[\"ticker\"].unique()\n    sector_map = get_sector_map(tickers)\n    df = df.join(sector_map, on=\"ticker\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:46:45.204070Z","iopub.execute_input":"2025-12-05T08:46:45.204334Z","iopub.status.idle":"2025-12-05T08:46:45.209323Z","shell.execute_reply.started":"2025-12-05T08:46:45.204315Z","shell.execute_reply":"2025-12-05T08:46:45.208688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = add_sector(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:46:45.210205Z","iopub.execute_input":"2025-12-05T08:46:45.210572Z","iopub.status.idle":"2025-12-05T08:47:56.425989Z","shell.execute_reply.started":"2025-12-05T08:46:45.210552Z","shell.execute_reply":"2025-12-05T08:47:56.424882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_adv_dollar(df):\n    adv_df = get_adv_dollar(df)\n    \n    df = df.merge(\n        adv_df,\n        on=[\"Date\", \"ticker\"],\n        how=\"left\",\n    )\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:47:56.427702Z","iopub.execute_input":"2025-12-05T08:47:56.428049Z","iopub.status.idle":"2025-12-05T08:47:56.433701Z","shell.execute_reply.started":"2025-12-05T08:47:56.428019Z","shell.execute_reply":"2025-12-05T08:47:56.432772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = add_adv_dollar(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:47:56.434640Z","iopub.execute_input":"2025-12-05T08:47:56.434978Z","iopub.status.idle":"2025-12-05T08:47:56.963532Z","shell.execute_reply.started":"2025-12-05T08:47:56.434952Z","shell.execute_reply":"2025-12-05T08:47:56.962578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Change Date formats to UTC and normalize (remove time component) to ensure matching\ndf['Date'] = pd.to_datetime(df['Date'], utc=True).dt.normalize()\nleg_one_inds['Date'] = pd.to_datetime(leg_one_inds['Date'], utc=True).dt.normalize()\n\n# Select only the new features\njoin_keys = ['ticker', 'Date']\nnew_features = [col for col in leg_one_inds.columns if col not in df.columns or col in join_keys]\nleg_one_clean = leg_one_inds[new_features].copy()\n\n# Remove duplicate rows in the indicators (Safety Check)\nleg_one_clean = leg_one_clean.drop_duplicates(subset=join_keys)\n\n# Merge\ndf = df.merge(\n    leg_one_clean, \n    on=join_keys, \n    how='left'\n)\n\n# Verification\nprint(f\"New shape: {df.shape}\")\nprint(\"New columns added:\", list(set(df.columns) - set(leg_one_inds.columns).symmetric_difference(set(new_features))))\n# (Simple print of added columns for sanity check)\nprint(\"Added Columns:\", [c for c in new_features if c not in join_keys])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:47:56.965559Z","iopub.execute_input":"2025-12-05T08:47:56.965867Z","iopub.status.idle":"2025-12-05T08:47:57.401946Z","shell.execute_reply.started":"2025-12-05T08:47:56.965826Z","shell.execute_reply":"2025-12-05T08:47:57.401096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show all columns (prevents the \"...\" in the middle)\npd.set_option('display.max_columns', None)\n\n# Increase the width to see full content in each cell\npd.set_option('display.max_colwidth', None)\n\n# Show more rows (BE CAREFUL: setting this to None for 400k+ rows will freeze your browser)\n# Set it to a manageable number like 100 or 500\npd.set_option('display.max_rows', 100)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('final_dataset.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:47:57.402776Z","iopub.execute_input":"2025-12-05T08:47:57.403081Z","iopub.status.idle":"2025-12-05T08:48:18.736374Z","shell.execute_reply.started":"2025-12-05T08:47:57.403057Z","shell.execute_reply":"2025-12-05T08:48:18.733052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tickers.to_csv('top_tickers.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:48:18.740713Z","iopub.execute_input":"2025-12-05T08:48:18.741302Z","iopub.status.idle":"2025-12-05T08:48:18.753963Z","shell.execute_reply.started":"2025-12-05T08:48:18.741261Z","shell.execute_reply":"2025-12-05T08:48:18.753190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_return_data(\"/kaggle/working/final_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:48:18.755261Z","iopub.execute_input":"2025-12-05T08:48:18.755508Z","iopub.status.idle":"2025-12-05T08:48:23.218558Z","shell.execute_reply.started":"2025-12-05T08:48:18.755489Z","shell.execute_reply":"2025-12-05T08:48:23.217721Z"}},"outputs":[],"execution_count":null}]}