{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone GitHub Repo and Install Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-07T18:07:50.595015Z",
     "iopub.status.busy": "2025-12-07T18:07:50.594203Z",
     "iopub.status.idle": "2025-12-07T18:07:54.988662Z",
     "shell.execute_reply": "2025-12-07T18:07:54.987821Z",
     "shell.execute_reply.started": "2025-12-07T18:07:50.594948Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'multimodal-eq-sizing'...\n",
      "remote: Enumerating objects: 843, done.\u001b[K\n",
      "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
      "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
      "remote: Total 843 (delta 123), reused 93 (delta 93), pack-reused 681 (from 3)\u001b[K\n",
      "Receiving objects: 100% (843/843), 886.58 KiB | 14.78 MiB/s, done.\n",
      "Resolving deltas: 100% (518/518), done.\n",
      "Requirement already satisfied: yfinance==0.2.66 in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 1)) (0.2.66)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: datetime in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 5)) (0.6)\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 6)) (3.7.4.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 7)) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r multimodal-eq-sizing/requirements.txt (line 8)) (4.53.3)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (3.18.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (6.33.0)\n",
      "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r multimodal-eq-sizing/requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r multimodal-eq-sizing/requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: zope.interface in /usr/local/lib/python3.11/dist-packages (from datetime->-r multimodal-eq-sizing/requirements.txt (line 4)) (8.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.7)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2025.10.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->-r multimodal-eq-sizing/requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r multimodal-eq-sizing/requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r multimodal-eq-sizing/requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2024.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance==0.2.66->-r multimodal-eq-sizing/requirements.txt (line 1)) (2.23)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->-r multimodal-eq-sizing/requirements.txt (line 3)) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!rm -rf multimodal-eq-sizing\n",
    "!git clone https://github.com/brianrp09232000/multimodal-eq-sizing.git\n",
    "!pip install -r multimodal-eq-sizing/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:40:30.329882Z",
     "iopub.status.busy": "2025-12-07T20:40:30.329573Z",
     "iopub.status.idle": "2025-12-07T20:40:31.098553Z",
     "shell.execute_reply": "2025-12-07T20:40:31.097906Z",
     "shell.execute_reply.started": "2025-12-07T20:40:30.329855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:40:37.703241Z",
     "iopub.status.busy": "2025-12-07T20:40:37.702477Z",
     "iopub.status.idle": "2025-12-07T20:40:37.708749Z",
     "shell.execute_reply": "2025-12-07T20:40:37.708150Z",
     "shell.execute_reply.started": "2025-12-07T20:40:37.703217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(invalid=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:40:42.426510Z",
     "iopub.status.busy": "2025-12-07T20:40:42.426225Z",
     "iopub.status.idle": "2025-12-07T20:40:42.431210Z",
     "shell.execute_reply": "2025-12-07T20:40:42.430267Z",
     "shell.execute_reply.started": "2025-12-07T20:40:42.426488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Uses the current directory where the notebook is running\n",
    "repo_root = pathlib.Path(\"multimodal-eq-sizing\")\n",
    "sys.path.append(str(repo_root.resolve())) # .resolve() gets the full absolute path locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:41:18.340157Z",
     "iopub.status.busy": "2025-12-07T20:41:18.339082Z",
     "iopub.status.idle": "2025-12-07T20:41:18.344560Z",
     "shell.execute_reply": "2025-12-07T20:41:18.343795Z",
     "shell.execute_reply.started": "2025-12-07T20:41:18.340123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.data.loaders import (\n",
    "    get_tickers_history,\n",
    "    get_return_data,\n",
    "    get_excess_return,\n",
    "    get_vix_data,\n",
    "    get_spread_z,\n",
    "    get_sector_map,\n",
    "    get_adv_dollar\n",
    ")\n",
    "\n",
    "from src.data.features.price_features import calculate_leg_one_features\n",
    "from src.data.features.news_features import built_news_features\n",
    "from src.data.universe import tickers_with_most_headlines\n",
    "from src.data.universe import tickers_with_most_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:41:22.721619Z",
     "iopub.status.busy": "2025-12-07T20:41:22.720857Z",
     "iopub.status.idle": "2025-12-07T20:41:22.741243Z",
     "shell.execute_reply": "2025-12-07T20:41:22.740367Z",
     "shell.execute_reply.started": "2025-12-07T20:41:22.721595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_headlines_per_ticker(news_df, start=None, end=None):\n",
    "    \"\"\"Counts the number of headlines for each ticker symbol \n",
    "    Input: news_df pandas dataframe with ticker column for ticker symbols\n",
    "    Output: pandas dataframe containing two columns: ticker names and the \n",
    "                number of headlines for the ticker\"\"\"\n",
    "    \n",
    "    #check columns in dataframe\n",
    "    columns = list(news_df.columns)\n",
    "    if (('date' not in columns) and ('Date' not in columns)) or (('ticker' not in columns) and ('Stock_symbol' not in columns)):\n",
    "        print('input dataframe does not have both ticker and date columns')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    #find column names\n",
    "    date_col = 'date' if 'date' in columns else 'Date'\n",
    "    ticker_col = 'ticker' if 'ticker' in columns else 'Stock_symbol'\n",
    "\n",
    "    #filter dates\n",
    "    if start is not None: \n",
    "        start_filter = news_df[date_col] >= str(start)\n",
    "        news_df = news_df[start_filter]\n",
    "    if end is not None: \n",
    "        end_filter = news_df[date_col] <= str(end)\n",
    "        news_df = news_df[end_filter]\n",
    "    \n",
    "    # Count occurrences in a specific column\n",
    "    headline_counts = news_df[ticker_col].value_counts()\n",
    "    df = headline_counts.to_frame(name='count')\n",
    "    df['ticker'] = list(df.index)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df[['ticker','count']]\n",
    "\n",
    "\n",
    "\n",
    "def tickers_with_most_headlines(news_df, start=None, end=None, n=200):\n",
    "    \"\"\"Finds the tickers with the most headlines \n",
    "    Input: news_df pandas dataframe with ticker column for ticker symbols\n",
    "            optional: n interger, number of top tickers to return\n",
    "    Output: list containing the number of headlines per ticker\n",
    "                for the tickers with the most headlines\"\"\"\n",
    "    \n",
    "    #count headlines for each ticker\n",
    "    df = count_headlines_per_ticker(news_df, start, end)\n",
    "\n",
    "    #limit dataframe to n tickers\n",
    "    df = df.sort_values(['count'], ascending=False)\n",
    "    df = df[:n]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_date_range(df: pd.DataFrame) -> tuple:\n",
    "    grouped_by_date = df.groupby([\"ticker\"]).agg(['min', 'max', 'count'])[\"Date\"]\n",
    "    start = grouped_by_date[\"min\"].min()\n",
    "    end = grouped_by_date[\"max\"].max()\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def add_sector(df):\n",
    "    tickers = df[\"ticker\"].unique()\n",
    "    sector_map = get_sector_map(tickers)\n",
    "    df = df.join(sector_map, on=\"ticker\")\n",
    "    \n",
    "    # remove None values in 'sector', these are ETFs not stocks\n",
    "    df = df.dropna(subset=['sector'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_excess_return(df, start, end):\n",
    "    excess_return_df = get_excess_return(df, start, end)\n",
    "    df = df.merge(excess_return_df, on=[\"ticker\", \"Date\"], how=\"left\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_vix_z(df, start, end):\n",
    "    vix_z_df = get_vix_data(start, end)\n",
    "    format_str = \"%Y-%m-%d\"\n",
    "    vix_z_df[\"Date\"] = vix_z_df[\"Date\"].dt.strftime(format_str)\n",
    "    df[\"Date\"] = df[\"Date\"].dt.strftime(format_str) \n",
    "    df = df.merge(vix_z_df, on=[\"Date\"], how=\"left\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_spread_z(existing_df: pd.DataFrame, buffer_days=380) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use existing OHLCV df, pull buffered history, compute young-safe spread_z on the combined\n",
    "    Then merge back only the target window rows to prevent nulls.\n",
    "    \"\"\"\n",
    "    df = existing_df.copy()\n",
    "    start, end = df[\"Date\"].min(), df[\"Date\"].max()\n",
    "\n",
    "    tickers = sorted(df['ticker'].unique())\n",
    "    fetch_start = start - timedelta(days=buffer_days)\n",
    "    fetch_end   = end\n",
    "\n",
    "    # You already have get_tickers_history(tickers, start, end)\n",
    "    hist = get_tickers_history(tickers, fetch_start, fetch_end)\n",
    "    hist[\"Date\"] = pd.to_datetime(hist[\"Date\"], utc=True)\n",
    "\n",
    "    # Combine buffer + existing; keep existing rows on overlap\n",
    "    combined = pd.concat([hist, df], ignore_index=True)\n",
    "    combined = combined.sort_values(['ticker', \"Date\"])\n",
    "    combined = combined.drop_duplicates(subset=['ticker', \"Date\"], keep=\"last\")\n",
    "\n",
    "    # Compute young-safe spread_z on the full combined range\n",
    "    combined = get_spread_z(combined)\n",
    "\n",
    "    # Merge only computed columns back to target window\n",
    "    cols_to_merge = ['ticker', 'Date', \"spread_z\"]\n",
    "    out = df.merge(combined[cols_to_merge], on=['ticker', 'Date'], how=\"left\")\n",
    "\n",
    "    # Final minimal, causal clean-up to guarantee NON-NULL spread_z in target window:\n",
    "    # 1) per-ticker forward-fill (past only), 2) same-day cross-section median, 3) final 0\n",
    "    out[\"spread_z\"] = (\n",
    "        out.groupby('ticker')[\"spread_z\"].ffill()\n",
    "           .fillna(out.groupby('Date')[\"spread_z\"].transform(\"median\"))\n",
    "           .fillna(0.0)\n",
    "    ).clip(-3, 3)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_adv_dollar(df):\n",
    "    adv_df = get_adv_dollar(df)\n",
    "    \n",
    "    df = df.merge(\n",
    "        adv_df,\n",
    "        on=[\"Date\", \"ticker\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def count_headlines_all_days(news_df):\n",
    "    \"\"\"Counts the number of headlines for each ticker symbol each day\n",
    "    Input: news_df pandas dataframe with ticker column for ticker symbols and date for the headline date\n",
    "    Output: pandas dataframe containing the number of headlines per ticker per day\n",
    "                indexes are dates in string and tickers as the column names\"\"\"\n",
    "    \n",
    "    #check columns in dataframe\n",
    "    columns = list(news_df.columns)\n",
    "    if (('date' not in columns) and ('Date' not in columns)) or (('ticker' not in columns) and ('Stock_symbol' not in columns)):\n",
    "        print('input dataframe does not have both ticker and date columns')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    #find column names\n",
    "    date_col = 'date' if 'date' in columns else 'Date'\n",
    "    ticker_col = 'ticker' if 'ticker' in columns else 'Stock_symbol'\n",
    "    \n",
    "    # Count occurrences in the date column\n",
    "    headline_dates = news_df[date_col]#.str[:10]#.value_counts()\n",
    "    df = pd.DataFrame({ticker_col: news_df[ticker_col],\n",
    "                       date_col: headline_dates})\n",
    "    \n",
    "    # count headlines per day per ticker\n",
    "    df = df.groupby([date_col, ticker_col]).size().unstack(fill_value=0)\n",
    "    \n",
    "    #create list of dates needed\n",
    "    format_code = \"%Y-%m-%d\"# Corresponds to 'YYYY-MM-DD'\n",
    "    set_of_dates = set(df.index)\n",
    "    date_min = start#datetime.strptime(min('2010-01-04',min(set_of_dates)), format_code).date() #datetime(2000,1,1).date()#\n",
    "    date_max = end #datetime.strptime(max('2018-12-28',max(set_of_dates)), format_code).date()\n",
    "    date_lst = [(date_min+timedelta(i)) for i in range(int((date_max-date_min).days)+1)]\n",
    "    \n",
    "    #find dates not in dataframe\n",
    "    missing_dates = dict([(day,int(0)) for day in set(date_lst).difference(set(df.index))])\n",
    "    \n",
    "    #add missing dates to dataframe\n",
    "    tickers = list(set(df.columns))\n",
    "    tickers.sort()\n",
    "    empty_dict = dict([(ticker, missing_dates) for ticker in tickers])\n",
    "    add_dates = pd.DataFrame(empty_dict)\n",
    "    df = pd.concat([df, add_dates], ignore_index=False)\n",
    "    \n",
    "    #sort rows and columns\n",
    "    df = df.sort_index()\n",
    "    df = df.T\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_news_flag(news_df, price_df):\n",
    "    \"\"\"adds a new news flag column: 0=no news, 1=news\n",
    "    input: news_df with 'date', 'ticker', and other columns\n",
    "            price_df with 'Date', 'ticker', and other columns\n",
    "            optional start and end Timestamps\n",
    "    output: dataframe df\n",
    "    \"\"\"\n",
    "\n",
    "    #count headlines per ticker per day\n",
    "    news_count = count_headlines_all_days(news_df)\n",
    "    \n",
    "    #filter count_df by date\n",
    "    news_count = news_count.T\n",
    "    news_count['date'] = pd.to_datetime(list(news_count.index), utc=True)\n",
    "    \n",
    "    #convert news_count df to different format\n",
    "    news_cols = list(news_count.columns)\n",
    "    news_count = news_count.melt(id_vars=['date'], value_vars=news_cols, \n",
    "                  var_name='ticker', value_name='news flag')\n",
    "\n",
    "    # change count to flag: 0=no news, 1=news\n",
    "    news_count['news flag'] = [flag if flag < 2 else 1 for flag in news_count['news flag']]\n",
    "    news_count['date'] = pd.to_datetime(news_count['date'], utc=True)\n",
    "    news_count.sort_values(['date','ticker'], inplace=True)\n",
    "    \n",
    "    #add news flag: 0=no news, 1=news\n",
    "    price_df = pd.merge(price_df, news_count, left_on=['Date','ticker'], \n",
    "              right_on=['date','ticker'])\n",
    "\n",
    "    return price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading/Sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:10:18.064829Z",
     "iopub.status.busy": "2025-12-07T18:10:18.064059Z",
     "iopub.status.idle": "2025-12-07T18:10:18.199793Z",
     "shell.execute_reply": "2025-12-07T18:10:18.199175Z",
     "shell.execute_reply.started": "2025-12-07T18:10:18.064800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nasdaq-news/nasdaq_news.csv\n",
      "['news-trading', 'nasdaq-news']\n"
     ]
    }
   ],
   "source": [
    "# Get dataset from Kaggle Hub\n",
    "import kagglehub\n",
    "dir_path = kagglehub.dataset_download(\"zeroadamantium/nasdaq-news-articles\", force_download=True)\n",
    "file_name = \"nasdaq_news.csv\"\n",
    "path = os.path.join(dir_path, file_name)\n",
    "print(path)\n",
    "\n",
    "# This will print all the folders in the input directory\n",
    "print(os.listdir(\"/kaggle/input\"))\n",
    "\n",
    "# Once you see the folder name above, replace 'YOUR_FOLDER_NAME' below to see the files inside\n",
    "# print(os.listdir(\"/kaggle/input/YOUR_FOLDER_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:10:18.201060Z",
     "iopub.status.busy": "2025-12-07T18:10:18.200815Z",
     "iopub.status.idle": "2025-12-07T18:12:07.076821Z",
     "shell.execute_reply": "2025-12-07T18:12:07.076116Z",
     "shell.execute_reply.started": "2025-12-07T18:10:18.201042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filename = \"/kaggle/input/nasdaq-news/nasdaq_news.csv\"\n",
    "news_df = pd.read_csv(filename)\n",
    "news_df = news_df.drop(columns=['Article'])\n",
    "\n",
    "# 1. Check Local (Git/PC) first\n",
    "# if os.path.exists(filename):\n",
    "#     news_df = pd.read_csv(filename)\n",
    "#     print(f\"Success: Loaded {filename} from local folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:12:07.084567Z",
     "iopub.status.busy": "2025-12-07T18:12:07.084336Z",
     "iopub.status.idle": "2025-12-07T18:12:08.834257Z",
     "shell.execute_reply": "2025-12-07T18:12:08.833389Z",
     "shell.execute_reply.started": "2025-12-07T18:12:07.084550Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "start = pd.Timestamp('2010-01-04 05:00:00+0000', tz='UTC')\n",
    "end   = pd.Timestamp('2018-12-28 05:00:00+0000', tz='UTC')\n",
    "\n",
    "# Limit news to start and stop times\n",
    "news_df['Date'] = pd.to_datetime(list(news_df['Date']), utc=True)\n",
    "news_df = news_df[news_df['Date'] >= start]\n",
    "news_df = news_df[news_df['Date'] <= end]\n",
    "print(news_df.shape)\n",
    "\n",
    "tickers = tickers_with_most_headlines(news_df, str(start), str(end), 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Pricing Dataset (Yahoo Finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:12:08.836282Z",
     "iopub.status.busy": "2025-12-07T18:12:08.836035Z",
     "iopub.status.idle": "2025-12-07T18:12:57.918072Z",
     "shell.execute_reply": "2025-12-07T18:12:57.917155Z",
     "shell.execute_reply.started": "2025-12-07T18:12:08.836264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:yfinance:$X: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$DISH: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$WBA: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$FL: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$SPWR: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00) (Yahoo error = \"Data doesn't exist for startDate = 1262581200, endDate = 1546059600\")\n",
      "ERROR:yfinance:$BRK: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00)\n",
      "ERROR:yfinance:$DFS: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$PXD: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$AI: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00) (Yahoo error = \"Data doesn't exist for startDate = 1262581200, endDate = 1546059600\")\n",
      "ERROR:yfinance:$MRO: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$AMTD: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00) (Yahoo error = \"Data doesn't exist for startDate = 1262581200, endDate = 1546059600\")\n",
      "ERROR:yfinance:$BIG: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$HES: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$HA: possibly delisted; no timezone found\n",
      "ERROR:yfinance:$CONN: possibly delisted; no price data found  (1d 2010-01-04 05:00:00+00:00 -> 2018-12-29 05:00:00+00:00)\n",
      "ERROR:yfinance:$SWN: possibly delisted; no timezone found\n",
      "/kaggle/working/multimodal-eq-sizing/src/data/loaders.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(tickers_history_dfs)\n"
     ]
    }
   ],
   "source": [
    "# Get yfinance ticker history for all tickers in tickers df\n",
    "# yfinance will produce the \"possibly delisted\" message for tickers without information\n",
    "df = get_tickers_history(list(tickers['ticker']), start=start, end=end)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(['Capital Gains','Adj Close'], axis=1)\n",
    "\n",
    "# Limit df to only 200 tickers and tickers with data\n",
    "keep_tickers = list(df['ticker'].drop_duplicates()[:200])\n",
    "df = df[df['ticker'].isin(keep_tickers)]\n",
    "tickers = tickers[tickers['ticker'].isin(keep_tickers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Pricing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:12:57.919366Z",
     "iopub.status.busy": "2025-12-07T18:12:57.919124Z",
     "iopub.status.idle": "2025-12-07T18:13:53.894201Z",
     "shell.execute_reply": "2025-12-07T18:13:53.893506Z",
     "shell.execute_reply.started": "2025-12-07T18:12:57.919348Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391851, 10)\n"
     ]
    }
   ],
   "source": [
    "# 1. Add Sector\n",
    "df = add_sector(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:13:53.895403Z",
     "iopub.status.busy": "2025-12-07T18:13:53.894941Z",
     "iopub.status.idle": "2025-12-07T18:13:54.912708Z",
     "shell.execute_reply": "2025-12-07T18:13:54.911983Z",
     "shell.execute_reply.started": "2025-12-07T18:13:53.895383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391851, 19)\n"
     ]
    }
   ],
   "source": [
    "# 2. Add Leg One Indicators\n",
    "leg_one_inds = calculate_leg_one_features(df)\n",
    "print(leg_one_inds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:13:54.913665Z",
     "iopub.status.busy": "2025-12-07T18:13:54.913474Z",
     "iopub.status.idle": "2025-12-07T18:13:55.260803Z",
     "shell.execute_reply": "2025-12-07T18:13:55.260017Z",
     "shell.execute_reply.started": "2025-12-07T18:13:54.913651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391851, 12)\n"
     ]
    }
   ],
   "source": [
    "# 3. Add Excess Return\n",
    "df = add_excess_return(df, start, end)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:13:55.262077Z",
     "iopub.status.busy": "2025-12-07T18:13:55.261731Z",
     "iopub.status.idle": "2025-12-07T18:13:57.918549Z",
     "shell.execute_reply": "2025-12-07T18:13:57.917810Z",
     "shell.execute_reply.started": "2025-12-07T18:13:55.262057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay!ðŸ¥³\n",
      "(391851, 14)\n"
     ]
    }
   ],
   "source": [
    "# 4. Add VIX_Z\n",
    "df = add_vix_z(df, start, end)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:13:57.919726Z",
     "iopub.status.busy": "2025-12-07T18:13:57.919371Z",
     "iopub.status.idle": "2025-12-07T18:14:30.124295Z",
     "shell.execute_reply": "2025-12-07T18:14:30.123227Z",
     "shell.execute_reply.started": "2025-12-07T18:13:57.919703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391851, 15)\n"
     ]
    }
   ],
   "source": [
    "# 5. Add SPREAD_Z\n",
    "df = add_spread_z(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:14:30.126837Z",
     "iopub.status.busy": "2025-12-07T18:14:30.126605Z",
     "iopub.status.idle": "2025-12-07T18:14:30.421805Z",
     "shell.execute_reply": "2025-12-07T18:14:30.420999Z",
     "shell.execute_reply.started": "2025-12-07T18:14:30.126819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391851, 17)\n"
     ]
    }
   ],
   "source": [
    "# 6. Add ADV_DOLLAR\n",
    "df = add_adv_dollar(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:14:30.423195Z",
     "iopub.status.busy": "2025-12-07T18:14:30.422893Z",
     "iopub.status.idle": "2025-12-07T18:14:30.663411Z",
     "shell.execute_reply": "2025-12-07T18:14:30.662742Z",
     "shell.execute_reply.started": "2025-12-07T18:14:30.423176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape: (391851, 26)\n",
      "New columns added: ['vol_parkinson_20d', 'dollar_volume', 'VIX_Close', 'r1', 'r5', 'mom_rank', 'spread_z', 'o2c_return', 'VIX_z', 'ticker', 'Date', 'r10', 'excess_return', 'mom_12_1', 'adv_dollar', 'spy_r1', 'trend_ema_diff', 'vol_realized_20d']\n",
      "Added Columns: ['r1', 'r5', 'r10', 'trend_ema_diff', 'vol_realized_20d', 'vol_parkinson_20d', 'mom_12_1', 'mom_rank', 'spy_r1']\n"
     ]
    }
   ],
   "source": [
    "# 7. Preprocessing and Clean up\n",
    "\n",
    "# Change Date formats to UTCdf['Date'] = pd.to_datetime(df['Date'], utc=True).dt.normalize()\n",
    "leg_one_inds['Date'] = pd.to_datetime(leg_one_inds['Date'], utc=True).dt.normalize()\n",
    "\n",
    "# Select only the new features\n",
    "join_keys = ['ticker', 'Date']\n",
    "new_features = [col for col in leg_one_inds.columns if col not in df.columns or col in join_keys]\n",
    "leg_one_clean = leg_one_inds[new_features].copy()\n",
    "\n",
    "# Remove duplicate rows in the indicators\n",
    "leg_one_clean = leg_one_clean.drop_duplicates(subset=join_keys)\n",
    "\n",
    "# Merge\n",
    "df = df.merge(\n",
    "    leg_one_clean, \n",
    "    on=join_keys, \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verification\n",
    "print(f\"New shape: {df.shape}\")\n",
    "print(\"New columns added:\", list(set(df.columns) - set(leg_one_inds.columns).symmetric_difference(set(new_features))))\n",
    "# (Simple print of added columns for sanity check)\n",
    "print(\"Added Columns:\", [c for c in new_features if c not in join_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:14:30.664503Z",
     "iopub.status.busy": "2025-12-07T18:14:30.664148Z",
     "iopub.status.idle": "2025-12-07T18:14:30.692924Z",
     "shell.execute_reply": "2025-12-07T18:14:30.692013Z",
     "shell.execute_reply.started": "2025-12-07T18:14:30.664485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. Add NEXT_DAY EXCESS RETURN\n",
    "df[\"next_day_excess_return\"] = df.groupby('ticker')['excess_return'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:14:30.694141Z",
     "iopub.status.busy": "2025-12-07T18:14:30.693819Z",
     "iopub.status.idle": "2025-12-07T18:14:30.708762Z",
     "shell.execute_reply": "2025-12-07T18:14:30.708176Z",
     "shell.execute_reply.started": "2025-12-07T18:14:30.694122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391851, 28)\n"
     ]
    }
   ],
   "source": [
    "# 9. Add flags train/val/test flags to identify split sets\n",
    "df['split'] = 'train'\n",
    "df.loc[df['Date'] >=\"2015-01-01\", \"split\"] = \"val\"\n",
    "df.loc[df['Date'] >= \"2017-01-01\", \"split\"] = 'test'\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:14:30.709910Z",
     "iopub.status.busy": "2025-12-07T18:14:30.709575Z",
     "iopub.status.idle": "2025-12-07T18:16:14.704782Z",
     "shell.execute_reply": "2025-12-07T18:16:14.704022Z",
     "shell.execute_reply.started": "2025-12-07T18:14:30.709883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391686, 30)\n"
     ]
    }
   ],
   "source": [
    "# 10. Add NEWS_FLAG\n",
    "df = add_news_flag(news_df, df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:23:30.401093Z",
     "iopub.status.busy": "2025-12-07T18:23:30.400433Z",
     "iopub.status.idle": "2025-12-07T18:23:30.430768Z",
     "shell.execute_reply": "2025-12-07T18:23:30.430207Z",
     "shell.execute_reply.started": "2025-12-07T18:23:30.401068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sector</th>\n",
       "      <th>...</th>\n",
       "      <th>trend_ema_diff</th>\n",
       "      <th>vol_realized_20d</th>\n",
       "      <th>vol_parkinson_20d</th>\n",
       "      <th>mom_12_1</th>\n",
       "      <th>mom_rank</th>\n",
       "      <th>spy_r1</th>\n",
       "      <th>next_day_excess_return</th>\n",
       "      <th>split</th>\n",
       "      <th>date</th>\n",
       "      <th>news flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-05 00:00:00+00:00</td>\n",
       "      <td>53.698801</td>\n",
       "      <td>54.428219</td>\n",
       "      <td>53.664070</td>\n",
       "      <td>53.941940</td>\n",
       "      <td>13469263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>-0.006825</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-05 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-06 00:00:00+00:00</td>\n",
       "      <td>53.941910</td>\n",
       "      <td>54.254516</td>\n",
       "      <td>53.629303</td>\n",
       "      <td>53.664040</td>\n",
       "      <td>11573422.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.043662</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-06 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-07 00:00:00+00:00</td>\n",
       "      <td>53.768241</td>\n",
       "      <td>57.241645</td>\n",
       "      <td>53.594575</td>\n",
       "      <td>56.442760</td>\n",
       "      <td>38701038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-07 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-08 00:00:00+00:00</td>\n",
       "      <td>56.651173</td>\n",
       "      <td>57.971062</td>\n",
       "      <td>56.512232</td>\n",
       "      <td>57.658455</td>\n",
       "      <td>24019636.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-08 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-11 00:00:00+00:00</td>\n",
       "      <td>58.457350</td>\n",
       "      <td>58.631016</td>\n",
       "      <td>57.450062</td>\n",
       "      <td>58.214211</td>\n",
       "      <td>15999249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.014180</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-11 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date       Open       High        Low      Close  \\\n",
       "0 2010-01-05 00:00:00+00:00  53.698801  54.428219  53.664070  53.941940   \n",
       "1 2010-01-06 00:00:00+00:00  53.941910  54.254516  53.629303  53.664040   \n",
       "2 2010-01-07 00:00:00+00:00  53.768241  57.241645  53.594575  56.442760   \n",
       "3 2010-01-08 00:00:00+00:00  56.651173  57.971062  56.512232  57.658455   \n",
       "4 2010-01-11 00:00:00+00:00  58.457350  58.631016  57.450062  58.214211   \n",
       "\n",
       "       Volume  Dividends  Stock Splits ticker       sector  ...  \\\n",
       "0  13469263.0        0.0           0.0     GE  Industrials  ...   \n",
       "1  11573422.0        0.0           0.0     GE  Industrials  ...   \n",
       "2  38701038.0        0.0           0.0     GE  Industrials  ...   \n",
       "3  24019636.0        0.0           0.0     GE  Industrials  ...   \n",
       "4  15999249.0        0.0           0.0     GE  Industrials  ...   \n",
       "\n",
       "   trend_ema_diff  vol_realized_20d  vol_parkinson_20d  mom_12_1  mom_rank  \\\n",
       "0        0.032598               NaN                NaN       NaN       NaN   \n",
       "1        0.024568               NaN                NaN       NaN       NaN   \n",
       "2        0.344084               NaN                NaN       NaN       NaN   \n",
       "3        0.727206               NaN                NaN       NaN       NaN   \n",
       "4        1.077108               NaN                NaN       NaN       NaN   \n",
       "\n",
       "     spy_r1  next_day_excess_return  split                      date  \\\n",
       "0  0.002647               -0.006825  train 2010-01-05 00:00:00+00:00   \n",
       "1  0.000704                0.043662  train 2010-01-06 00:00:00+00:00   \n",
       "2  0.004221                0.011810  train 2010-01-07 00:00:00+00:00   \n",
       "3  0.003328               -0.001118  train 2010-01-08 00:00:00+00:00   \n",
       "4  0.001396                0.014180  train 2010-01-11 00:00:00+00:00   \n",
       "\n",
       "   news flag  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:43:29.273541Z",
     "iopub.status.busy": "2025-12-07T18:43:29.273229Z",
     "iopub.status.idle": "2025-12-07T18:43:29.669134Z",
     "shell.execute_reply": "2025-12-07T18:43:29.668290Z",
     "shell.execute_reply.started": "2025-12-07T18:43:29.273521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/datasets\n",
    "news_df.to_csv('/kaggle/working/datasets/filtered_news_dataset.csv', index=False)\n",
    "tickers.to_csv('/kaggle/working/datasets/top_tickers.csv', index=False)\n",
    "df.to_csv('/kaggle/working/datasets/prices_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T19:51:59.531883Z",
     "iopub.status.busy": "2025-12-07T19:51:59.531601Z",
     "iopub.status.idle": "2025-12-07T19:52:06.235176Z",
     "shell.execute_reply": "2025-12-07T19:52:06.234399Z",
     "shell.execute_reply.started": "2025-12-07T19:51:59.531866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Dataset https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news ...\n",
      "Starting upload for file /kaggle/working/datasets/prices_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199M/199M [00:02<00:00, 97.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/prices_dataset.csv (190MB)\n",
      "Starting upload for file /kaggle/working/datasets/filtered_news_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74.7M/74.7M [00:00<00:00, 99.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/filtered_news_dataset.csv (71MB)\n",
      "Starting upload for file /kaggle/working/datasets/top_tickers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.78k/1.78k [00:00<00:00, 8.85kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/top_tickers.csv (2KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news\n"
     ]
    }
   ],
   "source": [
    "import datetime as _dt\n",
    "import kagglehub\n",
    "\n",
    "# Save files to Kaggle Hub\n",
    "handle = \"zeroadamantium/multimodal-eq-sizing-output\"\n",
    "local_dataset_dir = \"/kaggle/working/datasets\"\n",
    "# current_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "kagglehub.dataset_upload(handle, local_dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add News Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T20:42:18.953118Z",
     "iopub.status.busy": "2025-12-07T20:42:18.952760Z",
     "iopub.status.idle": "2025-12-07T21:27:15.920799Z",
     "shell.execute_reply": "2025-12-07T21:27:15.920018Z",
     "shell.execute_reply.started": "2025-12-07T20:42:18.953094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NER model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder model for z_news (FinBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset in chunks...\n",
      "Found 1 CSV files:\n",
      "  - /kaggle/working/datasets/filtered_news_dataset.csv\n",
      "Processing chunk with 100000 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 100000 rows...\n",
      "Encoding headline embeddings (FinBERT) on 100000 rows...\n",
      "Running NER on column 'Article_title' (batched) for 100000 rows...\n",
      "Processing chunk with 2424 rows...\n",
      "Encoding headline embeddings (FinBERT) on 2424 rows...\n",
      "Running NER on column 'Article_title' (batched) for 2424 rows...\n",
      "Total rows processed across chunks: 802424\n",
      "Aggregating per (Stock_symbol, Date)...\n",
      "Computing novelty...\n",
      "Computing novelty in parallel for 3692 stocks with 4 workers...\n",
      "Aggregating embeddings into z_news (recency-weighted pooling)...\n",
      "[aggregate_z_news] Initialized attention vector w with dim=768\n",
      "Saving features to /kaggle/working/datasets/news_features.pkl ...\n",
      "Done.\n",
      "Columns in final output:\n",
      "Index(['Date', 'Stock_symbol', 'velocity', 'novelty', 'earnings_flag',\n",
      "       'guidance_flag', 'merger_flag', 'rating_flag', 'z_news',\n",
      "       'entities_today'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "news_features_df = built_news_features(\n",
    "    ner_text_column = \"Article_title\",\n",
    "    output_path = \"/kaggle/working/datasets/news_features.pkl\",\n",
    "    file_path = \"/kaggle/working/datasets/filtered_news_dataset.csv\",\n",
    "    chunk_size = 100_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:28:03.812067Z",
     "iopub.status.busy": "2025-12-07T21:28:03.811751Z",
     "iopub.status.idle": "2025-12-07T21:28:03.982281Z",
     "shell.execute_reply": "2025-12-07T21:28:03.981603Z",
     "shell.execute_reply.started": "2025-12-07T21:28:03.812042Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549379, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Save News Features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T21:31:28.035526Z",
     "iopub.status.busy": "2025-12-07T21:31:28.035167Z",
     "iopub.status.idle": "2025-12-07T22:12:41.365420Z",
     "shell.execute_reply": "2025-12-07T22:12:41.364792Z",
     "shell.execute_reply.started": "2025-12-07T21:31:28.035507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "news_features_df.to_csv(\"/kaggle/working/datasets/news_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:14:09.066203Z",
     "iopub.status.busy": "2025-12-07T22:14:09.065333Z",
     "iopub.status.idle": "2025-12-07T22:15:35.176336Z",
     "shell.execute_reply": "2025-12-07T22:15:35.175532Z",
     "shell.execute_reply.started": "2025-12-07T22:14:09.066177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Dataset https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news ...\n",
      "Starting upload for file /kaggle/working/datasets/news_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.90G/6.90G [01:09<00:00, 98.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/news_features.csv (6GB)\n",
      "Starting upload for file /kaggle/working/datasets/prices_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199M/199M [00:02<00:00, 98.4MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/prices_dataset.csv (190MB)\n",
      "Starting upload for file /kaggle/working/datasets/filtered_news_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.5M/75.5M [00:01<00:00, 71.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/filtered_news_dataset.csv (72MB)\n",
      "Starting upload for file /kaggle/working/datasets/top_tickers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.78k/1.78k [00:00<00:00, 8.57kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/top_tickers.csv (2KB)\n",
      "Starting upload for file /kaggle/working/datasets/news_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.76G/1.76G [00:07<00:00, 228MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/news_features.pkl (2GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news\n"
     ]
    }
   ],
   "source": [
    "import datetime as _dt\n",
    "import kagglehub\n",
    "\n",
    "# Save files to Kaggle Hub\n",
    "handle = \"zeroadamantium/multimodal-eq-sizing-output\"\n",
    "local_dataset_dir = \"/kaggle/working/datasets\"\n",
    "# current_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "kagglehub.dataset_upload(handle, local_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:30:21.751910Z",
     "iopub.status.busy": "2025-12-07T22:30:21.751288Z",
     "iopub.status.idle": "2025-12-07T22:30:21.768915Z",
     "shell.execute_reply": "2025-12-07T22:30:21.768010Z",
     "shell.execute_reply.started": "2025-12-07T22:30:21.751888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549379, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Stock_symbol</th>\n",
       "      <th>velocity</th>\n",
       "      <th>novelty</th>\n",
       "      <th>earnings_flag</th>\n",
       "      <th>guidance_flag</th>\n",
       "      <th>merger_flag</th>\n",
       "      <th>rating_flag</th>\n",
       "      <th>z_news</th>\n",
       "      <th>entities_today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-12 00:00:00+00:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.09942879, -0.7900245, -1.5315706, -0.03880...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-18 00:00:00+00:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.12355294, -0.22699751, -0.54240745, -0.222...</td>\n",
       "      <td>[ci, group, ti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-03 00:00:00+00:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.31843555, -0.499312, -1.284549, 0.3205093,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-16 00:00:00+00:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.32647607, -0.513768, -0.8571328, 1.0574433...</td>\n",
       "      <td>[bulls]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-02 00:00:00+00:00</td>\n",
       "      <td>AA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.57808304, -0.20705728, -1.0697136, 0.525321...</td>\n",
       "      <td>[citigroup, pandit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date Stock_symbol  velocity  novelty  earnings_flag  \\\n",
       "0 2010-01-12 00:00:00+00:00           AA         1      0.0              0   \n",
       "1 2010-01-18 00:00:00+00:00           AA         1      1.0              0   \n",
       "2 2010-02-03 00:00:00+00:00           AA         1      0.0              0   \n",
       "3 2010-02-16 00:00:00+00:00           AA         1      1.0              0   \n",
       "4 2010-03-02 00:00:00+00:00           AA         1      1.0              0   \n",
       "\n",
       "   guidance_flag  merger_flag  rating_flag  \\\n",
       "0              0            0            0   \n",
       "1              0            0            0   \n",
       "2              0            0            0   \n",
       "3              0            0            0   \n",
       "4              0            0            0   \n",
       "\n",
       "                                              z_news       entities_today  \n",
       "0  [-0.09942879, -0.7900245, -1.5315706, -0.03880...                   []  \n",
       "1  [-0.12355294, -0.22699751, -0.54240745, -0.222...      [ci, group, ti]  \n",
       "2  [-0.31843555, -0.499312, -1.284549, 0.3205093,...                   []  \n",
       "3  [-0.32647607, -0.513768, -0.8571328, 1.0574433...              [bulls]  \n",
       "4  [0.57808304, -0.20705728, -1.0697136, 0.525321...  [citigroup, pandit]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(news_features_df.shape)\n",
    "news_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Load Prices Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:32:22.198067Z",
     "iopub.status.busy": "2025-12-07T22:32:22.197702Z",
     "iopub.status.idle": "2025-12-07T22:32:22.220886Z",
     "shell.execute_reply": "2025-12-07T22:32:22.220240Z",
     "shell.execute_reply.started": "2025-12-07T22:32:22.198043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391686, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sector</th>\n",
       "      <th>...</th>\n",
       "      <th>trend_ema_diff</th>\n",
       "      <th>vol_realized_20d</th>\n",
       "      <th>vol_parkinson_20d</th>\n",
       "      <th>mom_12_1</th>\n",
       "      <th>mom_rank</th>\n",
       "      <th>spy_r1</th>\n",
       "      <th>next_day_excess_return</th>\n",
       "      <th>split</th>\n",
       "      <th>date</th>\n",
       "      <th>news flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-05 00:00:00+00:00</td>\n",
       "      <td>53.698801</td>\n",
       "      <td>54.428219</td>\n",
       "      <td>53.664070</td>\n",
       "      <td>53.941940</td>\n",
       "      <td>13469263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>-0.006825</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-05 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-06 00:00:00+00:00</td>\n",
       "      <td>53.941910</td>\n",
       "      <td>54.254516</td>\n",
       "      <td>53.629303</td>\n",
       "      <td>53.664040</td>\n",
       "      <td>11573422.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.043662</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-06 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-07 00:00:00+00:00</td>\n",
       "      <td>53.768241</td>\n",
       "      <td>57.241645</td>\n",
       "      <td>53.594575</td>\n",
       "      <td>56.442760</td>\n",
       "      <td>38701038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-07 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-08 00:00:00+00:00</td>\n",
       "      <td>56.651173</td>\n",
       "      <td>57.971062</td>\n",
       "      <td>56.512232</td>\n",
       "      <td>57.658455</td>\n",
       "      <td>24019636.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-08 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-11 00:00:00+00:00</td>\n",
       "      <td>58.457350</td>\n",
       "      <td>58.631016</td>\n",
       "      <td>57.450062</td>\n",
       "      <td>58.214211</td>\n",
       "      <td>15999249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GE</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.014180</td>\n",
       "      <td>train</td>\n",
       "      <td>2010-01-11 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date       Open       High        Low      Close  \\\n",
       "0  2010-01-05 00:00:00+00:00  53.698801  54.428219  53.664070  53.941940   \n",
       "1  2010-01-06 00:00:00+00:00  53.941910  54.254516  53.629303  53.664040   \n",
       "2  2010-01-07 00:00:00+00:00  53.768241  57.241645  53.594575  56.442760   \n",
       "3  2010-01-08 00:00:00+00:00  56.651173  57.971062  56.512232  57.658455   \n",
       "4  2010-01-11 00:00:00+00:00  58.457350  58.631016  57.450062  58.214211   \n",
       "\n",
       "       Volume  Dividends  Stock Splits ticker       sector  ...  \\\n",
       "0  13469263.0        0.0           0.0     GE  Industrials  ...   \n",
       "1  11573422.0        0.0           0.0     GE  Industrials  ...   \n",
       "2  38701038.0        0.0           0.0     GE  Industrials  ...   \n",
       "3  24019636.0        0.0           0.0     GE  Industrials  ...   \n",
       "4  15999249.0        0.0           0.0     GE  Industrials  ...   \n",
       "\n",
       "   trend_ema_diff  vol_realized_20d  vol_parkinson_20d  mom_12_1  mom_rank  \\\n",
       "0        0.032598               NaN                NaN       NaN       NaN   \n",
       "1        0.024568               NaN                NaN       NaN       NaN   \n",
       "2        0.344084               NaN                NaN       NaN       NaN   \n",
       "3        0.727206               NaN                NaN       NaN       NaN   \n",
       "4        1.077108               NaN                NaN       NaN       NaN   \n",
       "\n",
       "     spy_r1  next_day_excess_return  split                       date  \\\n",
       "0  0.002647               -0.006825  train  2010-01-05 00:00:00+00:00   \n",
       "1  0.000704                0.043662  train  2010-01-06 00:00:00+00:00   \n",
       "2  0.004221                0.011810  train  2010-01-07 00:00:00+00:00   \n",
       "3  0.003328               -0.001118  train  2010-01-08 00:00:00+00:00   \n",
       "4  0.001396                0.014180  train  2010-01-11 00:00:00+00:00   \n",
       "\n",
       "   news flag  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_dataset_df = pd.read_csv(\"/kaggle/working/datasets/prices_dataset.csv\")\n",
    "prices_dataset_df['Date'] = pd.to_datetime(prices_dataset_df['Date'], utc=True)\n",
    "print(prices_dataset_df.shape)\n",
    "prices_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset: Merge Prices and News Features Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:37:39.649893Z",
     "iopub.status.busy": "2025-12-07T22:37:39.649294Z",
     "iopub.status.idle": "2025-12-07T22:37:40.106856Z",
     "shell.execute_reply": "2025-12-07T22:37:40.106236Z",
     "shell.execute_reply.started": "2025-12-07T22:37:39.649870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_multimodal_eq_sizing_dataset_df = pd.merge(\n",
    "    prices_dataset_df,\n",
    "    news_features_df,\n",
    "    left_on = ['Date', 'ticker'],\n",
    "    right_on = ['Date', 'Stock_symbol'],\n",
    "    how = 'left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:50:55.536032Z",
     "iopub.status.busy": "2025-12-07T22:50:55.535248Z",
     "iopub.status.idle": "2025-12-07T22:50:55.660754Z",
     "shell.execute_reply": "2025-12-07T22:50:55.659881Z",
     "shell.execute_reply.started": "2025-12-07T22:50:55.535993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_multimodal_eq_sizing_dataset_df = processed_multimodal_eq_sizing_dataset_df.drop(columns=['Stock_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:51:23.846716Z",
     "iopub.status.busy": "2025-12-07T22:51:23.846440Z",
     "iopub.status.idle": "2025-12-07T22:51:23.880048Z",
     "shell.execute_reply": "2025-12-07T22:51:23.879341Z",
     "shell.execute_reply.started": "2025-12-07T22:51:23.846696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391686, 38)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sector</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>news flag</th>\n",
       "      <th>velocity</th>\n",
       "      <th>novelty</th>\n",
       "      <th>earnings_flag</th>\n",
       "      <th>guidance_flag</th>\n",
       "      <th>merger_flag</th>\n",
       "      <th>rating_flag</th>\n",
       "      <th>z_news</th>\n",
       "      <th>entities_today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79298</th>\n",
       "      <td>2018-03-21 00:00:00+00:00</td>\n",
       "      <td>14.012327</td>\n",
       "      <td>14.277348</td>\n",
       "      <td>13.955939</td>\n",
       "      <td>14.187128</td>\n",
       "      <td>6282800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EPD</td>\n",
       "      <td>Energy</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-03-21 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.22823562, -0.042468984, -0.74258715, -0.145...</td>\n",
       "      <td>[e, mp, ra energy, se, sr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237209</th>\n",
       "      <td>2014-09-25 00:00:00+00:00</td>\n",
       "      <td>8.955884</td>\n",
       "      <td>8.958680</td>\n",
       "      <td>8.841244</td>\n",
       "      <td>8.885982</td>\n",
       "      <td>17915100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CSX</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-09-25 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.21329702, -0.78418994, 0.33254468, -0.8496...</td>\n",
       "      <td>[cs, x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296435</th>\n",
       "      <td>2016-08-03 00:00:00+00:00</td>\n",
       "      <td>6.379677</td>\n",
       "      <td>6.616366</td>\n",
       "      <td>6.277718</td>\n",
       "      <td>6.503483</td>\n",
       "      <td>1983400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CC</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-08-03 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253505</th>\n",
       "      <td>2016-07-27 00:00:00+00:00</td>\n",
       "      <td>26.793909</td>\n",
       "      <td>27.254408</td>\n",
       "      <td>25.433670</td>\n",
       "      <td>25.596615</td>\n",
       "      <td>11405300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DVN</td>\n",
       "      <td>Energy</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-07-27 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-0.09879635, 0.087337136, 0.23949988, -0.8212...</td>\n",
       "      <td>[ana, ap, c, d, dar, devon energy, ko, vn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211375</th>\n",
       "      <td>2018-02-05 00:00:00+00:00</td>\n",
       "      <td>89.849939</td>\n",
       "      <td>91.084050</td>\n",
       "      <td>86.147615</td>\n",
       "      <td>86.276169</td>\n",
       "      <td>939500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CE</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>...</td>\n",
       "      <td>2018-02-05 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.21518224, -0.36966068, -1.3591042, -0.10072...</td>\n",
       "      <td>[ce, lane, ni plastics l. l. c, om, se]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date       Open       High        Low      Close  \\\n",
       "79298  2018-03-21 00:00:00+00:00  14.012327  14.277348  13.955939  14.187128   \n",
       "237209 2014-09-25 00:00:00+00:00   8.955884   8.958680   8.841244   8.885982   \n",
       "296435 2016-08-03 00:00:00+00:00   6.379677   6.616366   6.277718   6.503483   \n",
       "253505 2016-07-27 00:00:00+00:00  26.793909  27.254408  25.433670  25.596615   \n",
       "211375 2018-02-05 00:00:00+00:00  89.849939  91.084050  86.147615  86.276169   \n",
       "\n",
       "            Volume  Dividends  Stock Splits ticker           sector  ...  \\\n",
       "79298    6282800.0        0.0           0.0    EPD           Energy  ...   \n",
       "237209  17915100.0        0.0           0.0    CSX      Industrials  ...   \n",
       "296435   1983400.0        0.0           0.0     CC  Basic Materials  ...   \n",
       "253505  11405300.0        0.0           0.0    DVN           Energy  ...   \n",
       "211375    939500.0        0.0           0.0     CE  Basic Materials  ...   \n",
       "\n",
       "                             date  news flag  velocity  novelty  \\\n",
       "79298   2018-03-21 00:00:00+00:00          1       1.0      1.0   \n",
       "237209  2014-09-25 00:00:00+00:00          1       1.0      0.5   \n",
       "296435  2016-08-03 00:00:00+00:00          0       NaN      NaN   \n",
       "253505  2016-07-27 00:00:00+00:00          1       2.0      1.0   \n",
       "211375  2018-02-05 00:00:00+00:00          1       1.0      0.4   \n",
       "\n",
       "        earnings_flag  guidance_flag  merger_flag  rating_flag  \\\n",
       "79298             0.0            0.0          0.0          0.0   \n",
       "237209            1.0            0.0          0.0          0.0   \n",
       "296435            NaN            NaN          NaN          NaN   \n",
       "253505            1.0            0.0          0.0          0.0   \n",
       "211375            0.0            0.0          1.0          0.0   \n",
       "\n",
       "                                                   z_news  \\\n",
       "79298   [0.22823562, -0.042468984, -0.74258715, -0.145...   \n",
       "237209  [-0.21329702, -0.78418994, 0.33254468, -0.8496...   \n",
       "296435                                                NaN   \n",
       "253505  [-0.09879635, 0.087337136, 0.23949988, -0.8212...   \n",
       "211375  [0.21518224, -0.36966068, -1.3591042, -0.10072...   \n",
       "\n",
       "                                    entities_today  \n",
       "79298                   [e, mp, ra energy, se, sr]  \n",
       "237209                                     [cs, x]  \n",
       "296435                                         NaN  \n",
       "253505  [ana, ap, c, d, dar, devon energy, ko, vn]  \n",
       "211375     [ce, lane, ni plastics l. l. c, om, se]  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_multimodal_eq_sizing_dataset_df.shape)\n",
    "processed_multimodal_eq_sizing_dataset_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chekpoint: Save Final Multimodal Eq Sizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:54:51.996143Z",
     "iopub.status.busy": "2025-12-07T22:54:51.995393Z",
     "iopub.status.idle": "2025-12-07T22:54:54.025372Z",
     "shell.execute_reply": "2025-12-07T22:54:54.024546Z",
     "shell.execute_reply.started": "2025-12-07T22:54:51.996116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_multimodal_eq_sizing_dataset_df.to_pickle(\"/kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:54:54.027515Z",
     "iopub.status.busy": "2025-12-07T22:54:54.027256Z",
     "iopub.status.idle": "2025-12-07T22:54:54.030911Z",
     "shell.execute_reply": "2025-12-07T22:54:54.030163Z",
     "shell.execute_reply.started": "2025-12-07T22:54:54.027498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processed_multimodal_eq_sizing_dataset_df.to_csv(\"/kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:54:54.032033Z",
     "iopub.status.busy": "2025-12-07T22:54:54.031629Z",
     "iopub.status.idle": "2025-12-07T22:56:35.563768Z",
     "shell.execute_reply": "2025-12-07T22:56:35.563226Z",
     "shell.execute_reply.started": "2025-12-07T22:54:54.032010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Dataset https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news ...\n",
      "Starting upload for file /kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550M/550M [00:02<00:00, 185MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/proc_multimodal_eq_sizing_dataset.pkl (525MB)\n",
      "Starting upload for file /kaggle/working/datasets/news_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.90G/6.90G [01:21<00:00, 84.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/news_features.csv (6GB)\n",
      "Starting upload for file /kaggle/working/datasets/prices_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199M/199M [00:01<00:00, 110MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/prices_dataset.csv (190MB)\n",
      "Starting upload for file /kaggle/working/datasets/filtered_news_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.5M/75.5M [00:00<00:00, 93.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/filtered_news_dataset.csv (72MB)\n",
      "Starting upload for file /kaggle/working/datasets/top_tickers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.78k/1.78k [00:00<00:00, 8.86kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/top_tickers.csv (2KB)\n",
      "Starting upload for file /kaggle/working/datasets/news_features.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.76G/1.76G [00:07<00:00, 222MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: /kaggle/working/datasets/news_features.pkl (2GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your dataset has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/datasets/zeroadamantium/nasdaq-news\n"
     ]
    }
   ],
   "source": [
    "import datetime as _dt\n",
    "import kagglehub\n",
    "\n",
    "# Save files to Kaggle Hub\n",
    "handle = \"zeroadamantium/multimodal-eq-sizing-output\"\n",
    "local_dataset_dir = \"/kaggle/working/datasets\"\n",
    "# current_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "kagglehub.dataset_upload(handle, local_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T22:56:35.567154Z",
     "iopub.status.busy": "2025-12-07T22:56:35.566971Z",
     "iopub.status.idle": "2025-12-07T22:56:35.570535Z",
     "shell.execute_reply": "2025-12-07T22:56:35.569970Z",
     "shell.execute_reply.started": "2025-12-07T22:56:35.567140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get_return_data(\"/kaggle/working/final_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2976522,
     "sourceId": 5549253,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8870532,
     "sourceId": 13920741,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8902694,
     "sourceId": 13965676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
