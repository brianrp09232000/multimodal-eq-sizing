from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


@dataclass
class FQEConfig:
    """
    Hyperparameters for the FQE value network.
    """
    gamma: float = 0.99
    lr: float = 1e-3
    hidden_dim: int = 128
    batch_size: int = 1024
    epochs: int = 50
    shuffle_buffer: int = 100_000


class FQEValueNetTF:
    """
    Simple V(s) network trained with TD(0)-style regression:
        V(s) ≈ r + γ (1 - done) * V(s')

    This assumes the rewards are generated by the policy you want to evaluate
    (e.g. the dumb behavior policy). For a different target policy, treat this
    as an approximate off-policy evaluation.
    """

    def __init__(self, state_dim: int, config: FQEConfig):
        self.config = config
        self.state_dim = state_dim
        self.model = self._build_model(state_dim)
        self.optimizer = keras.optimizers.Adam(learning_rate=config.lr)

    def _build_model(self, state_dim: int) -> keras.Model:
        inputs = keras.Input(shape=(state_dim,), name="state")
        x = layers.Dense(self.config.hidden_dim, activation="relu")(inputs)
        x = layers.Dense(self.config.hidden_dim, activation="relu")(x)
        v = layers.Dense(1, name="v")(x)
        model = keras.Model(inputs=inputs, outputs=v, name="fqe_value_net")
        return model

    @tf.function
    def _train_step(
        self,
        s_b: tf.Tensor,
        r_b: tf.Tensor,
        ns_b: tf.Tensor,
        d_b: tf.Tensor,
    ) -> tf.Tensor:
        """
        One gradient step on a batch.

        s_b:   [B, state_dim]
        r_b:   [B, 1]
        ns_b:  [B, state_dim]
        d_b:   [B, 1]  (1.0 if terminal, 0.0 otherwise)
        """
        gamma = self.config.gamma

        with tf.GradientTape() as tape:
            v_s = self.model(s_b, training=True)     # [B, 1]
            v_ns = self.model(ns_b, training=True)   # [B, 1]

            target = r_b + gamma * (1.0 - d_b) * v_ns
            loss = tf.reduce_mean(tf.square(target - v_s))

        grads = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
        return loss

    def _compute_val_loss(
        self,
        val_dataset: tf.data.Dataset,
    ) -> float:
        """
        Compute validation loss on a dataset of (s, r, ns, d).
        """
        total_loss = 0.0
        n_batches = 0

        for s_b, r_b, ns_b, d_b in val_dataset:
            v_s = self.model(s_b, training=False)
            v_ns = self.model(ns_b, training=False)
            target = r_b + self.config.gamma * (1.0 - d_b) * v_ns
            loss = tf.reduce_mean(tf.square(target - v_s))
            total_loss += float(loss)
            n_batches += 1

        if n_batches == 0:
            return float("nan")
        return total_loss / n_batches

    def train(
        self,
        states: np.ndarray,
        rewards: np.ndarray,
        next_states: np.ndarray,
        dones: np.ndarray,
        val_data: Optional[
            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]
        ] = None,
    ) -> List[Dict]:
        """
        Train FQE on arrays.

        Parameters
        ----------
        states : np.ndarray [N, state_dim]
        rewards : np.ndarray [N, 1]
        next_states : np.ndarray [N, state_dim]
        dones : np.ndarray [N, 1]
        val_data : optional tuple of (states, rewards, next_states, dones)
            for monitoring validation TD loss.

        Returns
        -------
        logs : list of dicts with epoch, loss, val_loss (if provided).
        """
        cfg = self.config

        # Build training tf.data.Dataset
        train_ds = tf.data.Dataset.from_tensor_slices(
            (states, rewards, next_states, dones)
        )
        train_ds = train_ds.shuffle(
            buffer_size=min(cfg.shuffle_buffer, len(states)),
            reshuffle_each_iteration=True,
        ).batch(cfg.batch_size)

        val_ds = None
        if val_data is not None:
            vs, vr, vns, vd = val_data
            val_ds = tf.data.Dataset.from_tensor_slices(
                (vs, vr, vns, vd)
            ).batch(cfg.batch_size)

        logs: List[Dict] = []

        for epoch in range(cfg.epochs):
            total_loss = 0.0
            n_batches = 0

            for s_b, r_b, ns_b, d_b in train_ds:
                loss = self._train_step(s_b, r_b, ns_b, d_b)
                total_loss += float(loss)
                n_batches += 1

            avg_loss = total_loss / max(n_batches, 1)
            log = {"epoch": epoch, "loss": avg_loss}

            if val_ds is not None:
                val_loss = self._compute_val_loss(val_ds)
                log["val_loss"] = val_loss
                print(
                    f"FQE Epoch {epoch:03d}: loss={avg_loss:.6f}, "
                    f"val_loss={val_loss:.6f}"
                )
            else:
                print(f"FQE Epoch {epoch:03d}: loss={avg_loss:.6f}")

            logs.append(log)

        return logs

    def estimate_value(self, states: np.ndarray) -> float:
        """
        Estimate average V(s) over a set of states (e.g. starting states).

        Returns a scalar: mean V(s) across the rows.
        """
        if len(states) == 0:
            return float("nan")

        states_tf = tf.convert_to_tensor(states, dtype=tf.float32)
        v = self.model(states_tf, training=False).numpy().reshape(-1)
        return float(v.mean())


# ------------------------------
# Convenience helpers for rl_df
# ------------------------------

def build_fqe_arrays_from_rl_df(
    rl_df: pd.DataFrame,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:
    """
    Given rl_df from build_rl_dataset (with state_* / next_state_* / reward / done),
    build numpy arrays for FQE.

    Returns
    -------
    states        : [N, state_dim]
    rewards       : [N, 1]
    next_states   : [N, state_dim]
    dones         : [N, 1]
    state_cols    : List of state_* column names (for reference)
    """
    state_cols = [c for c in rl_df.columns if c.startswith("state_")]
    next_state_cols = [c for c in rl_df.columns if c.startswith("next_state_")]

    states = rl_df[state_cols].to_numpy().astype(np.float32)
    next_states = rl_df[next_state_cols].to_numpy().astype(np.float32)

    rewards = rl_df["reward"].to_numpy().astype(np.float32).reshape(-1, 1)
    dones = rl_df["done"].to_numpy().astype(np.float32).reshape(-1, 1)

    return states, rewards, next_states, dones, state_cols


def estimate_initial_value_by_ticker(
    fqe_model: FQEValueNetTF,
    rl_df: pd.DataFrame,
    state_cols: List[str],
) -> float:
    """
    Rough portfolio-level value estimate:
      - For each ticker, take its earliest state in rl_df
      - Evaluate V(s) and average across tickers

    This gives "average initial value per name" under the evaluated policy.
    """
    if "Date" not in rl_df.columns or "ticker" not in rl_df.columns:
        raise ValueError("rl_df must contain 'Date' and 'ticker' columns")

    df_sorted = rl_df.sort_values(["ticker", "Date"])
    first_states = df_sorted.groupby("ticker").head(1)
    states = first_states[state_cols].to_numpy().astype(np.float32)

    return fqe_model.estimate_value(states)
